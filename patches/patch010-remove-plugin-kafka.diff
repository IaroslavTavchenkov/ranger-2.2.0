diff --git a/plugin-kafka/.gitignore b/plugin-kafka/.gitignore
deleted file mode 100644
index ea8c4bf7f..000000000
--- a/plugin-kafka/.gitignore
+++ /dev/null
@@ -1 +0,0 @@
-/target
diff --git a/plugin-kafka/conf/kafka-ranger-env.sh b/plugin-kafka/conf/kafka-ranger-env.sh
deleted file mode 100755
index 605a1eb0d..000000000
--- a/plugin-kafka/conf/kafka-ranger-env.sh
+++ /dev/null
@@ -1,33 +0,0 @@
-#!/bin/bash
-
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-classpathmunge () {
-        escaped=`echo $1 | sed -e 's:\*:\\\\*:g'`
-        if ! echo ${CLASSPATH} | /bin/egrep -q "(^|:)${escaped}($|:)" ; then
-           if [ "$2" = "before" ] ; then
-              CLASSPATH=$1:${CLASSPATH}
-           else
-              CLASSPATH=${CLASSPATH}:$1
-           fi
-        fi
-}
-classpathmunge /etc/kafka/conf
-classpathmunge '/usr/hdp/current/hadoop-hdfs-client/*'
-classpathmunge '/usr/hdp/current/hadoop-hdfs-client/lib/*'
-classpathmunge '/etc/hadoop/conf'
-export CLASSPATH
-unset classpathmunge
diff --git a/plugin-kafka/conf/ranger-kafka-audit-changes.cfg b/plugin-kafka/conf/ranger-kafka-audit-changes.cfg
deleted file mode 100644
index dfd27f37c..000000000
--- a/plugin-kafka/conf/ranger-kafka-audit-changes.cfg
+++ /dev/null
@@ -1,68 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-xasecure.audit.hdfs.is.enabled                                     %XAAUDIT.HDFS.IS_ENABLED%                               mod create-if-not-exists
-xasecure.audit.hdfs.config.destination.directory                   %XAAUDIT.HDFS.DESTINATION_DIRECTORY%                    mod create-if-not-exists
-xasecure.audit.hdfs.config.destination.file                        %XAAUDIT.HDFS.DESTINTATION_FILE%                        mod create-if-not-exists
-xasecure.audit.hdfs.config.destination.flush.interval.seconds      %XAAUDIT.HDFS.DESTINTATION_FLUSH_INTERVAL_SECONDS%      mod create-if-not-exists
-xasecure.audit.hdfs.config.destination.rollover.interval.seconds   %XAAUDIT.HDFS.DESTINTATION_ROLLOVER_INTERVAL_SECONDS%   mod create-if-not-exists
-xasecure.audit.hdfs.config.destination.open.retry.interval.seconds %XAAUDIT.HDFS.DESTINTATION_OPEN_RETRY_INTERVAL_SECONDS% mod create-if-not-exists
-xasecure.audit.hdfs.config.local.buffer.directory                  %XAAUDIT.HDFS.LOCAL_BUFFER_DIRECTORY%                   mod create-if-not-exists
-xasecure.audit.hdfs.config.local.buffer.file                       %XAAUDIT.HDFS.LOCAL_BUFFER_FILE%                        mod create-if-not-exists
-xasecure.audit.hdfs.config.local.buffer.flush.interval.seconds     %XAAUDIT.HDFS.LOCAL_BUFFER_FLUSH_INTERVAL_SECONDS%      mod create-if-not-exists
-xasecure.audit.hdfs.config.local.buffer.rollover.interval.seconds  %XAAUDIT.HDFS.LOCAL_BUFFER_ROLLOVER_INTERVAL_SECONDS%   mod create-if-not-exists
-xasecure.audit.hdfs.config.local.archive.directory                 %XAAUDIT.HDFS.LOCAL_ARCHIVE_DIRECTORY%                  mod create-if-not-exists
-xasecure.audit.hdfs.config.local.archive.max.file.count            %XAAUDIT.HDFS.LOCAL_ARCHIVE_MAX_FILE_COUNT%             mod create-if-not-exists
-
-xasecure.audit.solr.is.enabled                                    %XAAUDIT.SOLR.IS_ENABLED%                               mod create-if-not-exists
-xasecure.audit.solr.solr_url                                      %XAAUDIT.SOLR.SOLR_URL%                                 mod create-if-not-exists
-
-#V3 configuration
-xasecure.audit.provider.summary.enabled				  %XAAUDIT.SUMMARY.ENABLE%				mod create-if-not-exists
-
-xasecure.audit.destination.solr                                    %XAAUDIT.SOLR.ENABLE%                               mod create-if-not-exists
-xasecure.audit.destination.solr.urls                               %XAAUDIT.SOLR.URL%                                 mod create-if-not-exists
-xasecure.audit.destination.solr.user %XAAUDIT.SOLR.USER% mod create-if-not-exists
-xasecure.audit.destination.solr.password %XAAUDIT.SOLR.PASSWORD% mod create-if-not-exists
-xasecure.audit.destination.solr.zookeepers                         %XAAUDIT.SOLR.ZOOKEEPER%                           mod create-if-not-exists
-xasecure.audit.destination.solr.batch.filespool.dir                %XAAUDIT.SOLR.FILE_SPOOL_DIR%                      mod create-if-not-exists
-
-xasecure.audit.destination.elasticsearch                                    %XAAUDIT.ELASTICSEARCH.ENABLE%                              mod create-if-not-exists
-xasecure.audit.destination.elasticsearch.urls                               %XAAUDIT.ELASTICSEARCH.URL%                                 mod create-if-not-exists
-xasecure.audit.destination.elasticsearch.user 							   %XAAUDIT.ELASTICSEARCH.USER% 								  mod create-if-not-exists
-xasecure.audit.destination.elasticsearch.password 						   %XAAUDIT.ELASTICSEARCH.PASSWORD% 							  mod create-if-not-exists
-xasecure.audit.destination.elasticsearch.index 						   %XAAUDIT.ELASTICSEARCH.INDEX% 							  mod create-if-not-exists
-xasecure.audit.destination.elasticsearch.port 						   %XAAUDIT.ELASTICSEARCH.PORT% 							  mod create-if-not-exists
-xasecure.audit.destination.elasticsearch.protocol 						   %XAAUDIT.ELASTICSEARCH.PROTOCOL% 							  mod create-if-not-exists
-
-xasecure.audit.destination.hdfs					   %XAAUDIT.HDFS.ENABLE%                      mod create-if-not-exists
-xasecure.audit.destination.hdfs.batch.filespool.dir                %XAAUDIT.HDFS.FILE_SPOOL_DIR%                      mod create-if-not-exists
-xasecure.audit.destination.hdfs.dir                		   %XAAUDIT.HDFS.HDFS_DIR%                      mod create-if-not-exists
-
-AZURE.ACCOUNTNAME                                                                                                 %XAAUDIT.HDFS.AZURE_ACCOUNTNAME%            var
-xasecure.audit.destination.hdfs.config.fs.azure.shellkeyprovider.script                                           %XAAUDIT.HDFS.AZURE_SHELL_KEY_PROVIDER%     mod         create-if-not-exists
-xasecure.audit.destination.hdfs.config.fs.azure.account.key.%AZURE.ACCOUNTNAME%.blob.core.windows.net             %XAAUDIT.HDFS.AZURE_ACCOUNTKEY%             mod         create-if-not-exists
-xasecure.audit.destination.hdfs.config.fs.azure.account.keyprovider.%AZURE.ACCOUNTNAME%.blob.core.windows.net     %XAAUDIT.HDFS.AZURE_ACCOUNTKEY_PROVIDER%    mod         create-if-not-exists
-
-#xasecure.audit.destination.file					   %XAAUDIT.FILE.ENABLE%                      mod create-if-not-exists
-#xasecure.audit.destination.file.dir                		   %XAAUDIT.FILE.DIR%                      mod create-if-not-exists
-
-#log4j configuration
-xasecure.audit.log4j.is.enabled                %XAAUDIT.LOG4J.ENABLE%                      mod create-if-not-exists
-xasecure.audit.log4j.is.async                %XAAUDIT.LOG4J.IS_ASYNC%                      mod create-if-not-exists
-xasecure.audit.log4j.async.max.queue.size                %XAAUDIT.LOG4J.ASYNC.MAX.QUEUE.SIZE%                      mod create-if-not-exists
-xasecure.audit.log4j.async.max.flush.interval.ms                %XAAUDIT.LOG4J.ASYNC.MAX.FLUSH.INTERVAL.MS%                      mod create-if-not-exists
-xasecure.audit.destination.log4j                %XAAUDIT.LOG4J.DESTINATION.LOG4J%                      mod create-if-not-exists
-xasecure.audit.destination.log4j.logger                %XAAUDIT.LOG4J.DESTINATION.LOG4J.LOGGER%                      mod create-if-not-exists
diff --git a/plugin-kafka/conf/ranger-kafka-audit.xml b/plugin-kafka/conf/ranger-kafka-audit.xml
deleted file mode 100644
index b7b4806e6..000000000
--- a/plugin-kafka/conf/ranger-kafka-audit.xml
+++ /dev/null
@@ -1,217 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Licensed to the Apache Software Foundation (ASF) under one or more
-  contributor license agreements.  See the NOTICE file distributed with
-  this work for additional information regarding copyright ownership.
-  The ASF licenses this file to You under the Apache License, Version 2.0
-  (the "License"); you may not use this file except in compliance with
-  the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License.
--->
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<configuration xmlns:xi="http://www.w3.org/2001/XInclude">
-	<property>
-		<name>xasecure.audit.is.enabled</name>
-		<value>true</value>
-	</property>	
-	
-
-	<!-- HDFS audit provider configuration -->
-	<property>
-		<name>xasecure.audit.hdfs.is.enabled</name>
-		<value>false</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.is.async</name>
-		<value>true</value>
-	</property>	
-	
-	<property>
-		<name>xasecure.audit.hdfs.async.max.queue.size</name>
-		<value>1048576</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.async.max.flush.interval.ms</name>
-		<value>30000</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.encoding</name>
-		<value></value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.destination.directory</name>
-		<value>hdfs://NAMENODE_HOST:8020/ranger/audit/%app-type%/%time:yyyyMMdd%</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.destination.file</name>
-		<value>%hostname%-audit.log</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.destination.flush.interval.seconds</name>
-		<value>900</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.destination.rollover.interval.seconds</name>
-		<value>86400</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.destination.open.retry.interval.seconds</name>
-		<value>60</value>
-	</property>
-
-	<property>
-		<name>xasecure.audit.hdfs.config.local.buffer.directory</name>
-		<value>/var/log/kafka/audit</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.local.buffer.file</name>
-		<value>%time:yyyyMMdd-HHmm.ss%.log</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.local.buffer.file.buffer.size.bytes</name>
-		<value>8192</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.local.buffer.flush.interval.seconds</name>
-		<value>60</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.local.buffer.rollover.interval.seconds</name>
-		<value>600</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.local.archive.directory</name>
-		<value>/var/log/kafka/audit/archive</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.hdfs.config.local.archive.max.file.count</name>
-		<value>10</value>
-	</property>	
-	
-	<!-- Audit to HDFS on Azure Datastore (WASB) requires v3 style settings.  Comment the above and uncomment only the
-	following to audit to Azure Blob Datastore via hdfs' WASB schema.
-
-	NOTE: If you specify one audit destination in v3 style then other destinations, if any, must also be specified in v3 style
-	====
-
-	<property>
-		<name>xasecure.audit.destination.hdfs</name>
-		<value>enabled</value>
-	</property>
-
-	<property>
-		<name>xasecure.audit.destination.hdfs.dir</name>
-		<value>wasb://ranger-audit1@youraccount.blob.core.windows.net</value>
-	</property>
-
-	the following 3 correspond to the properties with similar name in core-site.xml, i.e.
-	- fs.azure.account.key.youraccount.blob.core.windows.net => xasecure.audit.destination.hdfs.config.fs.azure.account.key.youraccount.blob.core.windows.net and
-	- fs.azure.account.keyprovider.youraccount.blob.core.windows.net => xasecure.audit.destination.hdfs.config.fs.azure.account.keyprovider.youraccount.blob.core.windows.net,
-	- fs.azure.shellkeyprovider.script => xasecure.audit.destination.hdfs.config.fs.azure.shellkeyprovider.script,
-
-	<property>
-		<name>xasecure.audit.destination.hdfs.config.fs.azure.account.key.youraccount.blob.core.windows.net</name>
-		<value>YOUR ENCRYPTED ACCESS KEY</value>
-	</property>
-
-	<property>
-		<name>xasecure.audit.destination.hdfs.config.fs.azure.account.keyprovider.youraccount.blob.core.windows.net</name>
-		<value>org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider</value>
-	</property>
-
-	<property>
-		<name>xasecure.audit.destination.hdfs.config.fs.azure.shellkeyprovider.script</name>
-		<value>/usr/lib/python2.7/dist-packages/hdinsight_common/decrypt.sh</value>
-	</property>
-	-->
-
-	<!-- Log4j audit provider configuration -->
-	<property>
-		<name>xasecure.audit.log4j.is.enabled</name>
-		<value>false</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.log4j.is.async</name>
-		<value>false</value>
-	</property>	
-	
-	<property>
-		<name>xasecure.audit.log4j.async.max.queue.size</name>
-		<value>10240</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.log4j.async.max.flush.interval.ms</name>
-		<value>30000</value>
-	</property>	
-	
-	
-	<!-- Kafka audit provider configuration -->
-	<property>
-		<name>xasecure.audit.kafka.is.enabled</name>
-		<value>false</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.kafka.async.max.queue.size</name>
-		<value>1</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.kafka.async.max.flush.interval.ms</name>
-		<value>1000</value>
-	</property>	
-	
-	<property>
-		<name>xasecure.audit.kafka.broker_list</name>
-		<value>localhost:9092</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.kafka.topic_name</name>
-		<value>ranger_audits</value>
-	</property>	
-	
-	<!-- Ranger audit provider configuration -->
-	<property>
-		<name>xasecure.audit.solr.is.enabled</name>
-		<value>false</value>
-	</property>
-	
-	<property>
-		<name>xasecure.audit.solr.async.max.queue.size</name>
-		<value>1</value>
-	</property>	
-
-	<property>
-		<name>xasecure.audit.solr.async.max.flush.interval.ms</name>
-		<value>1000</value>
-	</property>	
-	
-	<property>
-		<name>xasecure.audit.solr.solr_url</name>
-		<value>http://localhost:6083/solr/ranger_audits</value>
-	</property>	
-</configuration>
diff --git a/plugin-kafka/conf/ranger-kafka-security-changes.cfg b/plugin-kafka/conf/ranger-kafka-security-changes.cfg
deleted file mode 100644
index 7eb98dfe8..000000000
--- a/plugin-kafka/conf/ranger-kafka-security-changes.cfg
+++ /dev/null
@@ -1,28 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# Change the original policy parameter to work with policy manager based.
-# 
-#
-ranger.plugin.kafka.service.name %REPOSITORY_NAME% mod create-if-not-exists
-
-ranger.plugin.kafka.policy.source.impl org.apache.ranger.admin.client.RangerAdminRESTClient mod create-if-not-exists
-
-ranger.plugin.kafka.policy.rest.url                %POLICY_MGR_URL%                          mod create-if-not-exists
-ranger.plugin.kafka.policy.rest.ssl.config.file    %COMPONENT_INSTALL_DIR_NAME%/config/ranger-policymgr-ssl.xml  mod create-if-not-exists
-ranger.plugin.kafka.policy.pollIntervalMs          30000                                     mod create-if-not-exists
-ranger.plugin.kafka.policy.cache.dir               %POLICY_CACHE_FILE_PATH%                  mod create-if-not-exists
-ranger.policy.rest.client.connection.timeoutMs	   120000									 mod create-if-not-exists
-ranger.policy.rest.client.read.timeoutMs		   30000									 mod create-if-not-exists
\ No newline at end of file
diff --git a/plugin-kafka/conf/ranger-kafka-security.xml b/plugin-kafka/conf/ranger-kafka-security.xml
deleted file mode 100644
index 2c06f5c15..000000000
--- a/plugin-kafka/conf/ranger-kafka-security.xml
+++ /dev/null
@@ -1,83 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Licensed to the Apache Software Foundation (ASF) under one or more
-  contributor license agreements.  See the NOTICE file distributed with
-  this work for additional information regarding copyright ownership.
-  The ASF licenses this file to You under the Apache License, Version 2.0
-  (the "License"); you may not use this file except in compliance with
-  the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License.
--->
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<configuration xmlns:xi="http://www.w3.org/2001/XInclude">
-	<property>
-		<name>ranger.plugin.kafka.service.name</name>
-		<value>kafkadev</value>
-		<description>
-			Name of the Ranger service containing policies for this KAFKA instance
-		</description>
-	</property>
-
-	<property>
-		<name>ranger.plugin.kafka.policy.source.impl</name>
-		<value>org.apache.ranger.admin.client.RangerAdminRESTClient</value>
-		<description>
-			Class to retrieve policies from the source
-		</description>
-	</property>
-
-	<property>
-		<name>ranger.plugin.kafka.policy.rest.url</name>
-		<value>http://policymanagerhost:port</value>
-		<description>
-			URL to Ranger Admin
-		</description>
-	</property>
-
-	<property>
-		<name>ranger.plugin.kafka.policy.rest.ssl.config.file</name>
-		<value>/etc/kafka/conf/ranger-policymgr-ssl.xml</value>
-		<description>
-			Path to the file containing SSL details to contact Ranger Admin
-		</description>
-	</property>
-
-	<property>
-		<name>ranger.plugin.kafka.policy.pollIntervalMs</name>
-		<value>30000</value>
-		<description>
-			How often to poll for changes in policies?
-		</description>
-	</property>
-
-	<property>
-		<name>ranger.plugin.kafka.policy.cache.dir</name>
-		<value>/etc/ranger/kafkadev/policycache</value>
-		<description>
-			Directory where Ranger policies are cached after successful retrieval from the source
-		</description>
-	</property>
-
-	<property>
-		<name>ranger.plugin.kafka.policy.rest.client.connection.timeoutMs</name>
-		<value>120000</value>
-		<description>
-			RangerRestClient Connection Timeout in Milli Seconds
-		</description>
-	</property>
-
-	<property>
-		<name>ranger.plugin.kafka.policy.rest.client.read.timeoutMs</name>
-		<value>30000</value>
-		<description>
-			RangerRestClient read Timeout in Milli Seconds
-		</description>
-	</property>
-</configuration>
diff --git a/plugin-kafka/conf/ranger-policymgr-ssl-changes.cfg b/plugin-kafka/conf/ranger-policymgr-ssl-changes.cfg
deleted file mode 100644
index 2d413929d..000000000
--- a/plugin-kafka/conf/ranger-policymgr-ssl-changes.cfg
+++ /dev/null
@@ -1,21 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# SSL Params
-#
-xasecure.policymgr.clientssl.keystore					 %SSL_KEYSTORE_FILE_PATH%						mod create-if-not-exists
-xasecure.policymgr.clientssl.keystore.credential.file	 jceks://file%CREDENTIAL_PROVIDER_FILE%			mod create-if-not-exists
-xasecure.policymgr.clientssl.truststore				     %SSL_TRUSTSTORE_FILE_PATH%						mod create-if-not-exists
-xasecure.policymgr.clientssl.truststore.credential.file  jceks://file%CREDENTIAL_PROVIDER_FILE%         mod create-if-not-exists	
diff --git a/plugin-kafka/conf/ranger-policymgr-ssl.xml b/plugin-kafka/conf/ranger-policymgr-ssl.xml
deleted file mode 100644
index a765a85ce..000000000
--- a/plugin-kafka/conf/ranger-policymgr-ssl.xml
+++ /dev/null
@@ -1,49 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Licensed to the Apache Software Foundation (ASF) under one or more
-  contributor license agreements.  See the NOTICE file distributed with
-  this work for additional information regarding copyright ownership.
-  The ASF licenses this file to You under the Apache License, Version 2.0
-  (the "License"); you may not use this file except in compliance with
-  the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License.
--->
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<configuration xmlns:xi="http://www.w3.org/2001/XInclude">
-	<!--  The following properties are used for 2-way SSL client server validation -->
-	<property>
-		<name>xasecure.policymgr.clientssl.keystore</name>
-		<value>kafkadev-clientcert.jks</value>
-		<description> 
-			Java Keystore files 
-		</description>
-	</property>
-	<property>
-		<name>xasecure.policymgr.clientssl.truststore</name>
-		<value>cacerts-xasecure.jks</value>
-		<description> 
-			java truststore file
-		</description>
-	</property>
-    <property>
-		<name>xasecure.policymgr.clientssl.keystore.credential.file</name>
-		<value>jceks://file/tmp/keystore-kafkadev-ssl.jceks</value>
-		<description> 
-			java  keystore credential file
-		</description>
-	</property>
-	<property>
-		<name>xasecure.policymgr.clientssl.truststore.credential.file</name>
-		<value>jceks://file/tmp/truststore-kafkadev-ssl.jceks</value>
-		<description> 
-			java  truststore credential file
-		</description>
-	</property>
-</configuration>
diff --git a/plugin-kafka/pom.xml b/plugin-kafka/pom.xml
deleted file mode 100644
index 8e3cf033d..000000000
--- a/plugin-kafka/pom.xml
+++ /dev/null
@@ -1,128 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!--
-  Licensed to the Apache Software Foundation (ASF) under one or more
-  contributor license agreements.  See the NOTICE file distributed with
-  this work for additional information regarding copyright ownership.
-  The ASF licenses this file to You under the Apache License, Version 2.0
-  (the "License"); you may not use this file except in compliance with
-  the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License.
--->
-
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
-    <modelVersion>4.0.0</modelVersion>
-    <artifactId>ranger-kafka-plugin</artifactId>
-    <name>KAFKA Security Plugin</name>
-    <description>KAFKA Security Plugin</description>
-    <packaging>jar</packaging>
-    <properties>
-        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
-    </properties>
-    <parent>
-        <groupId>org.apache.ranger</groupId>
-        <artifactId>ranger</artifactId>
-        <version>2.2.0</version>
-        <relativePath>..</relativePath>
-    </parent>
-    <dependencies>
-        <dependency>
-            <groupId>org.apache.ranger</groupId>
-            <artifactId>ranger-plugins-common</artifactId>
-            <version>${project.version}</version>
-        </dependency>
-        <dependency>
-            <groupId>org.apache.ranger</groupId>
-            <artifactId>ranger-plugins-audit</artifactId>
-            <version>${project.version}</version>
-        </dependency>
-        <dependency>
-            <groupId>org.apache.ranger</groupId>
-            <artifactId>credentialbuilder</artifactId>
-            <version>${project.version}</version>
-        </dependency>
-        <dependency>
-            <groupId>org.apache.kafka</groupId>
-            <artifactId>kafka_${scala.binary.version}</artifactId>
-            <version>${kafka.version}</version>
-        </dependency>
-        <dependency>
-            <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-hdfs</artifactId>
-            <version>${hadoop.version}</version>
-        </dependency>
-        <dependency>
-    		<groupId>org.apache.httpcomponents</groupId>
-    		<artifactId>httpcore</artifactId>
-    		<version>${httpcomponents.httpcore.version}</version>
-		</dependency>
-		<dependency>
-            <groupId>commons-codec</groupId>
-            <artifactId>commons-codec</artifactId>
-            <version>${commons.codec.version}</version>
-		</dependency>
-        <dependency>
-            <groupId>org.apache.curator</groupId>
-            <artifactId>curator-test</artifactId>
-            <version>${curator.test.version}</version>
-            <scope>test</scope>
-        </dependency>
-        <dependency>
-            <groupId>org.apache.kerby</groupId>
-            <artifactId>kerb-simplekdc</artifactId>
-            <version>${kerby.version}</version>
-            <scope>test</scope>
-        </dependency>
-        <dependency>
-            <groupId>org.bouncycastle</groupId>
-            <artifactId>bcpkix-jdk15on</artifactId>
-            <version>${bouncycastle.version}</version>
-            <scope>test</scope>
-        </dependency>
-        <dependency>
-            <groupId>junit</groupId>
-            <artifactId>junit</artifactId>
-        </dependency>
-        <dependency>
-            <groupId>com.fasterxml.jackson.core</groupId>
-            <artifactId>jackson-core</artifactId>
-            <version>2.10.4</version>
-            <scope>test</scope>
-        </dependency>
-    </dependencies>
-    <build>
-        <testResources>
-            <testResource>
-                <directory>src/test/resources</directory>
-                <includes>
-                    <include>**/*.xml</include>
-                    <include>log4j.properties</include>
-                </includes>
-                <filtering>true</filtering>
-            </testResource>
-            <testResource>
-                <directory>src/test/resources</directory>
-                <includes>
-                    <include>**/*.jks</include>
-                </includes>
-                <filtering>false</filtering>
-            </testResource>
-        </testResources>
-        <plugins>
-            <plugin>
-                <groupId>org.apache.maven.plugins</groupId>
-                <artifactId>maven-surefire-plugin</artifactId>
-                <inherited>true</inherited>
-                <configuration>
-                    <reuseForks>false</reuseForks>
-                </configuration>
-            </plugin>
-        </plugins>
-    </build>
-</project>
diff --git a/plugin-kafka/scripts/install.properties b/plugin-kafka/scripts/install.properties
deleted file mode 100644
index facbc790f..000000000
--- a/plugin-kafka/scripts/install.properties
+++ /dev/null
@@ -1,165 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-# Location of component folder
-COMPONENT_INSTALL_DIR_NAME=../kafka
-
-#
-# Location of Policy Manager URL  
-#
-# Example:
-# POLICY_MGR_URL=http://policymanager.xasecure.net:6080
-#
-POLICY_MGR_URL=
-
-#
-# This is the repository name created within policy manager
-#
-# Example:
-# REPOSITORY_NAME=kafkadev
-#
-REPOSITORY_NAME=
-
-# AUDIT configuration with V3 properties
-
-#Should audit be summarized at source
-XAAUDIT.SUMMARY.ENABLE=true
-
-# Enable audit logs to Solr
-#Example
-#XAAUDIT.SOLR.ENABLE=true
-#XAAUDIT.SOLR.URL=http://localhost:6083/solr/ranger_audits
-#XAAUDIT.SOLR.ZOOKEEPER=
-#XAAUDIT.SOLR.FILE_SPOOL_DIR=/var/log/kafka/audit/solr/spool
-
-XAAUDIT.SOLR.ENABLE=false
-XAAUDIT.SOLR.URL=NONE
-XAAUDIT.SOLR.USER=NONE
-XAAUDIT.SOLR.PASSWORD=NONE
-XAAUDIT.SOLR.ZOOKEEPER=NONE
-XAAUDIT.SOLR.FILE_SPOOL_DIR=/var/log/kafka/audit/solr/spool
-
-# Enable audit logs to ElasticSearch
-#Example
-#XAAUDIT.ELASTICSEARCH.ENABLE=true
-#XAAUDIT.ELASTICSEARCH.URL=localhost
-#XAAUDIT.ELASTICSEARCH.INDEX=audit
-
-XAAUDIT.ELASTICSEARCH.ENABLE=false
-XAAUDIT.ELASTICSEARCH.URL=NONE
-XAAUDIT.ELASTICSEARCH.USER=NONE
-XAAUDIT.ELASTICSEARCH.PASSWORD=NONE
-XAAUDIT.ELASTICSEARCH.INDEX=NONE
-XAAUDIT.ELASTICSEARCH.PORT=NONE
-XAAUDIT.ELASTICSEARCH.PROTOCOL=NONE
-
-# Enable audit logs to HDFS
-#Example
-#XAAUDIT.HDFS.ENABLE=true
-#XAAUDIT.HDFS.HDFS_DIR=hdfs://node-1.example.com:8020/ranger/audit
-#  If using Azure Blob Storage
-#XAAUDIT.HDFS.HDFS_DIR=wasb[s]://<containername>@<accountname>.blob.core.windows.net/<path>
-#XAAUDIT.HDFS.HDFS_DIR=wasb://ranger_audit_container@my-azure-account.blob.core.windows.net/ranger/audit
-#XAAUDIT.HDFS.FILE_SPOOL_DIR=/var/log/kafka/audit/hdfs/spool
-
-XAAUDIT.HDFS.ENABLE=false
-XAAUDIT.HDFS.HDFS_DIR=hdfs://__REPLACE__NAME_NODE_HOST:8020/ranger/audit
-XAAUDIT.HDFS.FILE_SPOOL_DIR=/var/log/kafka/audit/hdfs/spool
-
-# Following additional propertis are needed When auditing to Azure Blob Storage via HDFS
-# Get these values from your /etc/hadoop/conf/core-site.xml
-#XAAUDIT.HDFS.HDFS_DIR=wasb[s]://<containername>@<accountname>.blob.core.windows.net/<path>
-XAAUDIT.HDFS.AZURE_ACCOUNTNAME=__REPLACE_AZURE_ACCOUNT_NAME
-XAAUDIT.HDFS.AZURE_ACCOUNTKEY=__REPLACE_AZURE_ACCOUNT_KEY
-XAAUDIT.HDFS.AZURE_SHELL_KEY_PROVIDER=__REPLACE_AZURE_SHELL_KEY_PROVIDER
-XAAUDIT.HDFS.AZURE_ACCOUNTKEY_PROVIDER=__REPLACE_AZURE_ACCOUNT_KEY_PROVIDER
-
-#Log4j Audit Provider
-XAAUDIT.LOG4J.ENABLE=false
-XAAUDIT.LOG4J.IS_ASYNC=false
-XAAUDIT.LOG4J.ASYNC.MAX.QUEUE.SIZE=10240
-XAAUDIT.LOG4J.ASYNC.MAX.FLUSH.INTERVAL.MS=30000
-XAAUDIT.LOG4J.DESTINATION.LOG4J=true
-XAAUDIT.LOG4J.DESTINATION.LOG4J.LOGGER=xaaudit
-
-# End of V3 properties
-
-#
-#  Audit to HDFS Configuration
-#
-# If XAAUDIT.HDFS.IS_ENABLED is set to true, please replace tokens
-# that start with __REPLACE__ with appropriate values
-#  XAAUDIT.HDFS.IS_ENABLED=true
-#  XAAUDIT.HDFS.DESTINATION_DIRECTORY=hdfs://__REPLACE__NAME_NODE_HOST:8020/ranger/audit/%app-type%/%time:yyyyMMdd%
-#  XAAUDIT.HDFS.LOCAL_BUFFER_DIRECTORY=__REPLACE__LOG_DIR/kafka/audit
-#  XAAUDIT.HDFS.LOCAL_ARCHIVE_DIRECTORY=__REPLACE__LOG_DIR/kafka/audit/archive
-#
-# Example:
-#  XAAUDIT.HDFS.IS_ENABLED=true
-#  XAAUDIT.HDFS.DESTINATION_DIRECTORY=hdfs://namenode.example.com:8020/ranger/audit/%app-type%/%time:yyyyMMdd%
-#  XAAUDIT.HDFS.LOCAL_BUFFER_DIRECTORY=/var/log/kafka/audit
-#  XAAUDIT.HDFS.LOCAL_ARCHIVE_DIRECTORY=/var/log/kafka/audit/archive
-#
-XAAUDIT.HDFS.IS_ENABLED=false
-XAAUDIT.HDFS.DESTINATION_DIRECTORY=hdfs://__REPLACE__NAME_NODE_HOST:8020/ranger/audit/%app-type%/%time:yyyyMMdd%
-XAAUDIT.HDFS.LOCAL_BUFFER_DIRECTORY=__REPLACE__LOG_DIR/kafka/audit
-XAAUDIT.HDFS.LOCAL_ARCHIVE_DIRECTORY=__REPLACE__LOG_DIR/kafka/audit/archive
-
-XAAUDIT.HDFS.DESTINTATION_FILE=%hostname%-audit.log
-XAAUDIT.HDFS.DESTINTATION_FLUSH_INTERVAL_SECONDS=900
-XAAUDIT.HDFS.DESTINTATION_ROLLOVER_INTERVAL_SECONDS=86400
-XAAUDIT.HDFS.DESTINTATION_OPEN_RETRY_INTERVAL_SECONDS=60
-XAAUDIT.HDFS.LOCAL_BUFFER_FILE=%time:yyyyMMdd-HHmm.ss%.log
-XAAUDIT.HDFS.LOCAL_BUFFER_FLUSH_INTERVAL_SECONDS=60
-XAAUDIT.HDFS.LOCAL_BUFFER_ROLLOVER_INTERVAL_SECONDS=600
-XAAUDIT.HDFS.LOCAL_ARCHIVE_MAX_FILE_COUNT=10
-
-#Solr Audit Provider
-XAAUDIT.SOLR.IS_ENABLED=false
-XAAUDIT.SOLR.MAX_QUEUE_SIZE=1
-XAAUDIT.SOLR.MAX_FLUSH_INTERVAL_MS=1000
-XAAUDIT.SOLR.SOLR_URL=http://localhost:6083/solr/ranger_audits
-
-# End of V2 properties
-
-#
-# SSL Client Certificate Information
-#
-# Example:
-# SSL_KEYSTORE_FILE_PATH=/etc/hadoop/conf/ranger-plugin-keystore.jks
-# SSL_KEYSTORE_PASSWORD=none
-# SSL_TRUSTSTORE_FILE_PATH=/etc/hadoop/conf/ranger-plugin-truststore.jks
-# SSL_TRUSTSTORE_PASSWORD=none
-#
-# You do not need use SSL between agent and security admin tool, please leave these sample value as it is.
-#
-SSL_KEYSTORE_FILE_PATH=/etc/hadoop/conf/ranger-plugin-keystore.jks
-SSL_KEYSTORE_PASSWORD=myKeyFilePassword
-SSL_TRUSTSTORE_FILE_PATH=/etc/hadoop/conf/ranger-plugin-truststore.jks
-SSL_TRUSTSTORE_PASSWORD=changeit
-
-
-#
-# Custom component user
-# CUSTOM_COMPONENT_USER=<custom-user>
-# keep blank if component user is default
-CUSTOM_USER=kafka
-
-
-#
-# Custom component group
-# CUSTOM_COMPONENT_GROUP=<custom-group>
-# keep blank if component group is default
-CUSTOM_GROUP=hadoop
\ No newline at end of file
diff --git a/plugin-kafka/src/main/java/org/apache/ranger/authorization/kafka/authorizer/RangerKafkaAuditHandler.java b/plugin-kafka/src/main/java/org/apache/ranger/authorization/kafka/authorizer/RangerKafkaAuditHandler.java
deleted file mode 100644
index ee50e9539..000000000
--- a/plugin-kafka/src/main/java/org/apache/ranger/authorization/kafka/authorizer/RangerKafkaAuditHandler.java
+++ /dev/null
@@ -1,74 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-
-package org.apache.ranger.authorization.kafka.authorizer;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.ranger.audit.model.AuthzAuditEvent;
-import org.apache.ranger.plugin.audit.RangerDefaultAuditHandler;
-import org.apache.ranger.plugin.policyengine.RangerAccessRequest;
-import org.apache.ranger.plugin.policyengine.RangerAccessResourceImpl;
-import org.apache.ranger.plugin.policyengine.RangerAccessResult;
-
-public class RangerKafkaAuditHandler extends RangerDefaultAuditHandler {
-    private static final Log LOG = LogFactory.getLog(RangerKafkaAuditHandler.class);
-
-    private AuthzAuditEvent auditEvent      = null;
-
-    public RangerKafkaAuditHandler(){
-    }
-
-    @Override
-    public void processResult(RangerAccessResult result) {
-        // If Cluster Resource Level Topic Creation is not Allowed we don't audit.
-        // Subsequent call from Kafka for Topic Creation at Topic resource Level will be audited.
-        if (!isAuditingNeeded(result)) {
-            return;
-        }
-        auditEvent = super.getAuthzEvents(result);
-    }
-
-    private boolean isAuditingNeeded(final RangerAccessResult result) {
-        boolean ret = true;
-        boolean 			    isAllowed = result.getIsAllowed();
-        RangerAccessRequest request = result.getAccessRequest();
-        RangerAccessResourceImpl resource = (RangerAccessResourceImpl) request.getResource();
-        String resourceName 			  = (String) resource.getValue(RangerKafkaAuthorizer.KEY_CLUSTER);
-        if (resourceName != null) {
-            if (request.getAccessType().equalsIgnoreCase(RangerKafkaAuthorizer.ACCESS_TYPE_CREATE) && !isAllowed) {
-                ret = false;
-            }
-        }
-        return ret;
-    }
-
-    public void flushAudit() {
-        if(LOG.isDebugEnabled()) {
-            LOG.info("==> RangerYarnAuditHandler.flushAudit(" + "AuditEvent: " + auditEvent + ")");
-        }
-        if (auditEvent != null) {
-            super.logAuthzAudit(auditEvent);
-        }
-        if(LOG.isDebugEnabled()) {
-            LOG.info("<== RangerYarnAuditHandler.flushAudit(" + "AuditEvent: " + auditEvent + ")");
-        }
-    }
-}
diff --git a/plugin-kafka/src/main/java/org/apache/ranger/authorization/kafka/authorizer/RangerKafkaAuthorizer.java b/plugin-kafka/src/main/java/org/apache/ranger/authorization/kafka/authorizer/RangerKafkaAuthorizer.java
deleted file mode 100644
index 2a1b812e0..000000000
--- a/plugin-kafka/src/main/java/org/apache/ranger/authorization/kafka/authorizer/RangerKafkaAuthorizer.java
+++ /dev/null
@@ -1,328 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.ranger.authorization.kafka.authorizer;
-
-import java.util.Date;
-import java.util.Map;
-
-import org.apache.kafka.common.config.SaslConfigs;
-import org.apache.kafka.common.network.ListenerName;
-import org.apache.kafka.common.security.JaasContext;
-import org.apache.kafka.common.security.auth.SecurityProtocol;
-import scala.collection.immutable.HashSet;
-import scala.collection.immutable.Set;
-import kafka.security.auth.*;
-import kafka.network.RequestChannel.Session;
-
-import org.apache.commons.lang.StringUtils;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.kafka.common.security.auth.KafkaPrincipal;
-import org.apache.ranger.audit.provider.MiscUtil;
-import org.apache.ranger.plugin.policyengine.RangerAccessRequestImpl;
-import org.apache.ranger.plugin.policyengine.RangerAccessResourceImpl;
-import org.apache.ranger.plugin.policyengine.RangerAccessResult;
-import org.apache.ranger.plugin.service.RangerBasePlugin;
-import org.apache.ranger.plugin.util.RangerPerfTracer;
-
-public class RangerKafkaAuthorizer implements Authorizer {
-	private static final Log logger = LogFactory
-			.getLog(RangerKafkaAuthorizer.class);
-	private static final Log PERF_KAFKAAUTH_REQUEST_LOG = RangerPerfTracer.getPerfLogger("kafkaauth.request");
-
-	public static final String KEY_TOPIC = "topic";
-	public static final String KEY_CLUSTER = "cluster";
-	public static final String KEY_CONSUMER_GROUP = "consumergroup";
-	public static final String KEY_TRANSACTIONALID = "transactionalid";
-	public static final String KEY_DELEGATIONTOKEN = "delegationtoken";
-
-	public static final String ACCESS_TYPE_READ = "consume";
-	public static final String ACCESS_TYPE_WRITE = "publish";
-	public static final String ACCESS_TYPE_CREATE = "create";
-	public static final String ACCESS_TYPE_DELETE = "delete";
-	public static final String ACCESS_TYPE_CONFIGURE = "configure";
-	public static final String ACCESS_TYPE_DESCRIBE = "describe";
-	public static final String ACCESS_TYPE_DESCRIBE_CONFIGS = "describe_configs";
-	public static final String ACCESS_TYPE_ALTER_CONFIGS    = "alter_configs";
-	public static final String ACCESS_TYPE_IDEMPOTENT_WRITE = "idempotent_write";
-	public static final String ACCESS_TYPE_CLUSTER_ACTION   = "cluster_action";
-
-	private static volatile RangerBasePlugin rangerPlugin = null;
-	RangerKafkaAuditHandler auditHandler = null;
-
-	public RangerKafkaAuthorizer() {
-	}
-
-	/*
-	 * (non-Javadoc)
-	 *
-	 * @see kafka.security.auth.Authorizer#configure(Map<String, Object>)
-	 */
-	@Override
-	public void configure(Map<String, ?> configs) {
-		RangerBasePlugin me = rangerPlugin;
-		if (me == null) {
-			synchronized(RangerKafkaAuthorizer.class) {
-				me = rangerPlugin;
-				if (me == null) {
-					try {
-						// Possible to override JAAS configuration which is used by Ranger, otherwise
-						// SASL_PLAINTEXT is used, which force Kafka to use 'sasl_plaintext.KafkaServer',
-						// if it's not defined, then it reverts to 'KafkaServer' configuration.
-						final Object jaasContext = configs.get("ranger.jaas.context");
-						final String listenerName = (jaasContext instanceof String
-								&& StringUtils.isNotEmpty((String) jaasContext)) ? (String) jaasContext
-										: SecurityProtocol.SASL_PLAINTEXT.name();
-						final String saslMechanism = SaslConfigs.GSSAPI_MECHANISM;
-						JaasContext context = JaasContext.loadServerContext(new ListenerName(listenerName), saslMechanism, configs);
-						MiscUtil.setUGIFromJAASConfig(context.name());
-						logger.info("LoginUser=" + MiscUtil.getUGILoginUser());
-					} catch (Throwable t) {
-						logger.error("Error getting principal.", t);
-					}
-					me = rangerPlugin = new RangerBasePlugin("kafka", "kafka");
-				}
-			}
-		}
-		logger.info("Calling plugin.init()");
-		rangerPlugin.init();
-		auditHandler = new RangerKafkaAuditHandler();
-		rangerPlugin.setResultProcessor(auditHandler);
-	}
-
-	@Override
-	public void close() {
-		logger.info("close() called on authorizer.");
-		try {
-			if (rangerPlugin != null) {
-				rangerPlugin.cleanup();
-			}
-		} catch (Throwable t) {
-			logger.error("Error closing RangerPlugin.", t);
-		}
-	}
-
-	@Override
-	public boolean authorize(Session session, Operation operation,
-			Resource resource) {
-
-		if (rangerPlugin == null) {
-			MiscUtil.logErrorMessageByInterval(logger,
-					"Authorizer is still not initialized");
-			return false;
-		}
-
-		RangerPerfTracer perf = null;
-
-		if(RangerPerfTracer.isPerfTraceEnabled(PERF_KAFKAAUTH_REQUEST_LOG)) {
-			perf = RangerPerfTracer.getPerfTracer(PERF_KAFKAAUTH_REQUEST_LOG, "RangerKafkaAuthorizer.authorize(resource=" + resource + ")");
-		}
-		String userName = null;
-		if (session.principal() != null) {
-			userName = session.principal().getName();
-		}
-		java.util.Set<String> userGroups = MiscUtil
-				.getGroupsForRequestUser(userName);
-		String ip = session.clientAddress().getHostAddress();
-
-		// skip leading slash
-		if (StringUtils.isNotEmpty(ip) && ip.charAt(0) == '/') {
-			ip = ip.substring(1);
-		}
-
-		Date eventTime = new Date();
-		String accessType = mapToRangerAccessType(operation);
-		boolean validationFailed = false;
-		String validationStr = "";
-
-		if (accessType == null) {
-			if (MiscUtil.logErrorMessageByInterval(logger,
-					"Unsupported access type. operation=" + operation)) {
-				logger.fatal("Unsupported access type. session=" + session
-						+ ", operation=" + operation + ", resource=" + resource);
-			}
-			validationFailed = true;
-			validationStr += "Unsupported access type. operation=" + operation;
-		}
-		String action = accessType;
-
-		RangerAccessRequestImpl rangerRequest = new RangerAccessRequestImpl();
-		rangerRequest.setUser(userName);
-		rangerRequest.setUserGroups(userGroups);
-		rangerRequest.setClientIPAddress(ip);
-		rangerRequest.setAccessTime(eventTime);
-
-		RangerAccessResourceImpl rangerResource = new RangerAccessResourceImpl();
-		rangerRequest.setResource(rangerResource);
-		rangerRequest.setAccessType(accessType);
-		rangerRequest.setAction(action);
-		rangerRequest.setRequestData(resource.name());
-
-		if (resource.resourceType().equals(Topic$.MODULE$)) {
-			rangerResource.setValue(KEY_TOPIC, resource.name());
-		} else if (resource.resourceType().equals(Cluster$.MODULE$)) {
-			rangerResource.setValue(KEY_CLUSTER, resource.name());
-		} else if (resource.resourceType().equals(Group$.MODULE$)) {
-			rangerResource.setValue(KEY_CONSUMER_GROUP, resource.name());
-		} else if (resource.resourceType().equals(TransactionalId$.MODULE$)) {
-			rangerResource.setValue(KEY_TRANSACTIONALID, resource.name());
-		} else if (resource.resourceType().equals(DelegationToken$.MODULE$)) {
-			rangerResource.setValue(KEY_DELEGATIONTOKEN, resource.name());
-		} else {
-			logger.fatal("Unsupported resourceType=" + resource.resourceType());
-			validationFailed = true;
-		}
-
-		boolean returnValue = false;
-		if (validationFailed) {
-			MiscUtil.logErrorMessageByInterval(logger, validationStr
-					+ ", request=" + rangerRequest);
-		} else {
-
-			try {
-				RangerAccessResult result = rangerPlugin
-						.isAccessAllowed(rangerRequest);
-				if (result == null) {
-					logger.error("Ranger Plugin returned null. Returning false");
-				} else {
-					returnValue = result.getIsAllowed();
-				}
-			} catch (Throwable t) {
-				logger.error("Error while calling isAccessAllowed(). request="
-						+ rangerRequest, t);
-			} finally {
-				auditHandler.flushAudit();
-			}
-		}
-		RangerPerfTracer.log(perf);
-
-		if (logger.isDebugEnabled()) {
-			logger.debug("rangerRequest=" + rangerRequest + ", return="
-					+ returnValue);
-		}
-		return returnValue;
-	}
-
-	/*
-	 * (non-Javadoc)
-	 *
-	 * @see
-	 * kafka.security.auth.Authorizer#addAcls(scala.collection.immutable.Set,
-	 * kafka.security.auth.Resource)
-	 */
-	@Override
-	public void addAcls(Set<Acl> acls, Resource resource) {
-		logger.error("addAcls(Set<Acl>, Resource) is not supported by Ranger for Kafka");
-	}
-
-	/*
-	 * (non-Javadoc)
-	 *
-	 * @see
-	 * kafka.security.auth.Authorizer#removeAcls(scala.collection.immutable.Set,
-	 * kafka.security.auth.Resource)
-	 */
-	@Override
-	public boolean removeAcls(Set<Acl> acls, Resource resource) {
-		logger.error("removeAcls(Set<Acl>, Resource) is not supported by Ranger for Kafka");
-		return false;
-	}
-
-	/*
-	 * (non-Javadoc)
-	 *
-	 * @see
-	 * kafka.security.auth.Authorizer#removeAcls(kafka.security.auth.Resource)
-	 */
-	@Override
-	public boolean removeAcls(Resource resource) {
-		logger.error("removeAcls(Resource) is not supported by Ranger for Kafka");
-		return false;
-	}
-
-	/*
-	 * (non-Javadoc)
-	 *
-	 * @see kafka.security.auth.Authorizer#getAcls(kafka.security.auth.Resource)
-	 */
-	@Override
-	public Set<Acl> getAcls(Resource resource) {
-		Set<Acl> aclList = new HashSet<Acl>();
-		logger.error("getAcls(Resource) is not supported by Ranger for Kafka");
-
-		return aclList;
-	}
-
-	/*
-	 * (non-Javadoc)
-	 *
-	 * @see
-	 * kafka.security.auth.Authorizer#getAcls(kafka.security.auth.KafkaPrincipal
-	 * )
-	 */
-	@Override
-	public scala.collection.immutable.Map<Resource, Set<Acl>> getAcls(
-			KafkaPrincipal principal) {
-		scala.collection.immutable.Map<Resource, Set<Acl>> aclList = new scala.collection.immutable.HashMap<Resource, Set<Acl>>();
-		logger.error("getAcls(KafkaPrincipal) is not supported by Ranger for Kafka");
-		return aclList;
-	}
-
-	/*
-	 * (non-Javadoc)
-	 *
-	 * @see kafka.security.auth.Authorizer#getAcls()
-	 */
-	@Override
-	public scala.collection.immutable.Map<Resource, Set<Acl>> getAcls() {
-		scala.collection.immutable.Map<Resource, Set<Acl>> aclList = new scala.collection.immutable.HashMap<Resource, Set<Acl>>();
-		logger.error("getAcls() is not supported by Ranger for Kafka");
-		return aclList;
-	}
-
-	/**
-	 * @param operation
-	 * @return
-	 */
-	private String mapToRangerAccessType(Operation operation) {
-		if (operation.equals(Read$.MODULE$)) {
-			return ACCESS_TYPE_READ;
-		} else if (operation.equals(Write$.MODULE$)) {
-			return ACCESS_TYPE_WRITE;
-		} else if (operation.equals(Alter$.MODULE$)) {
-			return ACCESS_TYPE_CONFIGURE;
-		} else if (operation.equals(Describe$.MODULE$)) {
-			return ACCESS_TYPE_DESCRIBE;
-		} else if (operation.equals(ClusterAction$.MODULE$)) {
-			return ACCESS_TYPE_CLUSTER_ACTION;
-		} else if (operation.equals(Create$.MODULE$)) {
-			return ACCESS_TYPE_CREATE;
-		} else if (operation.equals(Delete$.MODULE$)) {
-			return ACCESS_TYPE_DELETE;
-		} else if (operation.equals(DescribeConfigs$.MODULE$)) {
-			return ACCESS_TYPE_DESCRIBE_CONFIGS;
-		} else if (operation.equals(AlterConfigs$.MODULE$)) {
-			return ACCESS_TYPE_ALTER_CONFIGS;
-		} else if (operation.equals(IdempotentWrite$.MODULE$)) {
-			return ACCESS_TYPE_IDEMPOTENT_WRITE;
-		}
-		return null;
-	}
-}
diff --git a/plugin-kafka/src/main/java/org/apache/ranger/services/kafka/RangerServiceKafka.java b/plugin-kafka/src/main/java/org/apache/ranger/services/kafka/RangerServiceKafka.java
deleted file mode 100644
index 836acd921..000000000
--- a/plugin-kafka/src/main/java/org/apache/ranger/services/kafka/RangerServiceKafka.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.ranger.services.kafka;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.commons.lang.StringUtils;
-import org.apache.ranger.plugin.model.RangerPolicy;
-import org.apache.ranger.plugin.model.RangerPolicy.RangerPolicyItem;
-import org.apache.ranger.plugin.model.RangerPolicy.RangerPolicyItemAccess;
-import org.apache.ranger.plugin.model.RangerService;
-import org.apache.ranger.plugin.model.RangerServiceDef;
-import org.apache.ranger.plugin.service.RangerBaseService;
-import org.apache.ranger.plugin.service.ResourceLookupContext;
-import org.apache.ranger.services.kafka.client.ServiceKafkaClient;
-import org.apache.ranger.services.kafka.client.ServiceKafkaConnectionMgr;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import static org.apache.ranger.plugin.policyengine.RangerPolicyEngine.GROUP_PUBLIC;
-
-public class RangerServiceKafka extends RangerBaseService {
-	private static final Log LOG = LogFactory.getLog(RangerServiceKafka.class);
-	public static final String ACCESS_TYPE_DESCRIBE = "describe";
-
-	public RangerServiceKafka() {
-		super();
-	}
-
-	@Override
-	public void init(RangerServiceDef serviceDef, RangerService service) {
-		super.init(serviceDef, service);
-	}
-
-	@Override
-	public Map<String, Object> validateConfig() throws Exception {
-		Map<String, Object> ret = new HashMap<String, Object>();
-
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("==> RangerServiceKafka.validateConfig(" + serviceName + ")");
-		}
-
-		if (configs != null) {
-			try {
-				ret = ServiceKafkaConnectionMgr.connectionTest(serviceName, configs);
-			} catch (Exception e) {
-				LOG.error("<== RangerServiceKafka.validateConfig Error:" + e);
-				throw e;
-			}
-		}
-
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("<== RangerServiceKafka.validateConfig(" + serviceName + "): ret=" + ret);
-		}
-
-		return ret;
-	}
-
-	@Override
-	public List<String> lookupResource(ResourceLookupContext context) throws Exception {
-		List<String> ret = null;
-
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("==> RangerServiceKafka.lookupResource(" + serviceName + ")");
-		}
-
-		if (configs != null) {
-			ServiceKafkaClient serviceKafkaClient = ServiceKafkaConnectionMgr.getKafkaClient(serviceName, configs);
-
-			ret = serviceKafkaClient.getResources(context);
-		}
-
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("<== RangerServiceKafka.lookupResource(" + serviceName + "): ret=" + ret);
-		}
-
-		return ret;
-	}
-
-	@Override
-	public List<RangerPolicy> getDefaultRangerPolicies() throws Exception {
-
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("==> RangerServiceKafka.getDefaultRangerPolicies() ");
-		}
-
-		List<RangerPolicy> ret = super.getDefaultRangerPolicies();
-
-		String authType = getConfig().get(RANGER_AUTH_TYPE,"simple");
-
-		if (StringUtils.equalsIgnoreCase(authType, KERBEROS_TYPE)) {
-			if (LOG.isDebugEnabled()) {
-				LOG.debug("Auth type is " + KERBEROS_TYPE);
-			}
-		} else {
-			if (LOG.isDebugEnabled()) {
-				LOG.debug("Auth type is " + authType);
-			}
-			for (RangerPolicy defaultPolicy : ret) {
-				if(defaultPolicy.getName().contains("all")){
-					for (RangerPolicy.RangerPolicyItem defaultPolicyItem : defaultPolicy.getPolicyItems()) {
-						defaultPolicyItem.getGroups().add(GROUP_PUBLIC);
-					}
-				}
-			}
-		}
-		for (RangerPolicy defaultPolicy : ret) {
-			if (defaultPolicy.getName().contains("all") && StringUtils.isNotBlank(lookUpUser)) {
-				RangerPolicyItem policyItemForLookupUser = new RangerPolicyItem();
-				policyItemForLookupUser.setUsers(Collections.singletonList(lookUpUser));
-				policyItemForLookupUser.setAccesses(Collections.singletonList(
-						new RangerPolicyItemAccess(ACCESS_TYPE_DESCRIBE)));
-				policyItemForLookupUser.setDelegateAdmin(false);
-				defaultPolicy.getPolicyItems().add(policyItemForLookupUser);
-			}
-		}
-
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("<== RangerServiceKafka.getDefaultRangerPolicies() ");
-		}
-		return ret;
-	}
-}
diff --git a/plugin-kafka/src/main/java/org/apache/ranger/services/kafka/client/ServiceKafkaClient.java b/plugin-kafka/src/main/java/org/apache/ranger/services/kafka/client/ServiceKafkaClient.java
deleted file mode 100644
index 11440815e..000000000
--- a/plugin-kafka/src/main/java/org/apache/ranger/services/kafka/client/ServiceKafkaClient.java
+++ /dev/null
@@ -1,245 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.ranger.services.kafka.client;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import java.util.concurrent.Callable;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.kafka.clients.admin.AdminClientConfig;
-import org.apache.kafka.clients.admin.AdminClient;
-import org.apache.kafka.clients.admin.KafkaAdminClient;
-import org.apache.kafka.clients.admin.TopicListing;
-import org.apache.kafka.clients.admin.ListTopicsResult;
-import org.apache.log4j.Logger;
-import org.apache.ranger.plugin.client.BaseClient;
-import org.apache.ranger.plugin.service.ResourceLookupContext;
-import org.apache.ranger.plugin.util.TimedEventUtil;
-
-public class ServiceKafkaClient {
-	private static final Logger LOG = Logger.getLogger(ServiceKafkaClient.class);
-
-	enum RESOURCE_TYPE {
-		TOPIC
-	}
-
-	String serviceName;
-	Map<String,String > configs;
-	private static final String errMessage = " You can still save the repository and start creating "
-			+ "policies, but you would not be able to use autocomplete for "
-			+ "resource names. Check server logs for more info.";
-
-	private static final String TOPIC_KEY				= "topic";
-	private static final long   LOOKUP_TIMEOUT_SEC		= 5;
-	private static final String KEY_SASL_MECHANISM		= "sasl.mechanism";
-	private static final String KEY_SASL_JAAS_CONFIG	= "sasl.jaas.config";
-	private static final String KEY_KAFKA_KEYTAB		= "kafka.keytab";
-	private static final String KEY_KAFKA_PRINCIPAL		= "kafka.principal";
-	private static final String JAAS_KRB5_MODULE		= "com.sun.security.auth.module.Krb5LoginModule required";
-	private static final String JAAS_USE_KEYTAB			= "useKeyTab=true";
-	private static final String JAAS_KEYTAB				= "keyTab=\"";
-	private static final String JAAS_STOKE_KEY			= "storeKey=true";
-	private static final String JAAS_SERVICE_NAME		= "serviceName=kafka";
-	private static final String JAAS_USER_TICKET_CACHE	= "useTicketCache=false";
-	private static final String JAAS_PRINCIPAL			= "principal=\"";
-
-	public ServiceKafkaClient(String serviceName, Map<String,String> configs) {
-		this.serviceName = serviceName;
-		this.configs = configs;
-	}
-
-	public Map<String, Object> connectionTest() {
-		String errMsg = errMessage;
-		Map<String, Object> responseData = new HashMap<String, Object>();
-		try {
-			getTopicList(null);
-			// If it doesn't throw exception, then assume the instance is
-			// reachable
-			String successMsg = "ConnectionTest Successful";
-			BaseClient.generateResponseDataMap(true, successMsg,
-					successMsg, null, null, responseData);
-		} catch (Exception e) {
-			LOG.error("Error connecting to Kafka. kafkaClient=" + this, e);
-			String failureMsg = "Unable to connect to Kafka instance."
-					+ e.getMessage();
-			BaseClient.generateResponseDataMap(false, failureMsg,
-					failureMsg + errMsg, null, null, responseData);
-		}
-		return responseData;
-	}
-
-	private List<String> getTopicList(List<String> ignoreTopicList) throws Exception {
-		List<String> ret = new ArrayList<String>();
-
-		int sessionTimeout = 5000;
-		int connectionTimeout = 10000;
-		AdminClient adminClient = null;
-
-		try {
-			Properties props = new Properties();
-			props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, configs.get(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG));
-			props.put(AdminClientConfig.SECURITY_PROTOCOL_CONFIG, configs.get(AdminClientConfig.SECURITY_PROTOCOL_CONFIG));
-			props.put(KEY_SASL_MECHANISM, configs.get(KEY_SASL_MECHANISM));
-			props.put(KEY_SASL_JAAS_CONFIG, getJAASConfig(configs));
-			props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, getIntProperty(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, sessionTimeout));
-			props.put(AdminClientConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG, getIntProperty(AdminClientConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG, connectionTimeout));
-			adminClient = KafkaAdminClient.create(props);
-			ListTopicsResult listTopicsResult = adminClient.listTopics();
-			if (listTopicsResult != null) {
-				Collection<TopicListing> topicListings = listTopicsResult.listings().get();
-				for (TopicListing topicListing : topicListings) {
-					String topicName = topicListing.name();
-					if (ignoreTopicList == null || !ignoreTopicList.contains(topicName)) {
-						ret.add(topicName);
-					}
-				}
-			}
-		} catch (Exception e) {
-			throw e;
-		} finally {
-			if (adminClient != null) {
-				adminClient.close();
-			}
-		}
-		return ret;
-	}
-
-
-
-	/**
-	 * @param context
-	 * @param context
-	 * @return
-	 */
-	public List<String> getResources(ResourceLookupContext context) {
-
-		String userInput = context.getUserInput();
-		String resource = context.getResourceName();
-		Map<String, List<String>> resourceMap = context.getResources();
-		List<String> resultList = null;
-		List<String> topicList = null;
-
-		RESOURCE_TYPE lookupResource = RESOURCE_TYPE.TOPIC;
-
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("<== getResources()  UserInput: \"" + userInput
-					+ "\" resource : " + resource + " resourceMap: "
-					+ resourceMap);
-		}
-
-		if (userInput != null && resource != null) {
-			if (resourceMap != null && !resourceMap.isEmpty()) {
-				topicList = resourceMap.get(TOPIC_KEY);
-			}
-			switch (resource.trim().toLowerCase()) {
-				case TOPIC_KEY:
-					lookupResource = RESOURCE_TYPE.TOPIC;
-					break;
-				default:
-					break;
-			}
-		}
-
-		if (userInput != null) {
-			try {
-				Callable<List<String>> callableObj = null;
-				final String userInputFinal = userInput;
-
-				final List<String> finalTopicList = topicList;
-
-				if (lookupResource == RESOURCE_TYPE.TOPIC) {
-					// get the topic list for given Input
-					callableObj = new Callable<List<String>>() {
-						@Override
-						public List<String> call() {
-							List<String> retList = new ArrayList<String>();
-							try {
-								List<String> list = getTopicList(finalTopicList);
-								if (userInputFinal != null
-										&& !userInputFinal.isEmpty()) {
-									for (String value : list) {
-										if (value.startsWith(userInputFinal)) {
-											retList.add(value);
-										}
-									}
-								} else {
-									retList.addAll(list);
-								}
-							} catch (Exception ex) {
-								LOG.error("Error getting topic.", ex);
-							}
-							return retList;
-						};
-					};
-				}
-				// If we need to do lookup
-				if (callableObj != null) {
-					synchronized (this) {
-						resultList = TimedEventUtil.timedTask(callableObj,
-								LOOKUP_TIMEOUT_SEC, TimeUnit.SECONDS);
-					}
-				}
-			} catch (Exception e) {
-				LOG.error("Unable to get hive resources.", e);
-			}
-		}
-
-		return resultList;
-	}
-
-	@Override
-	public String toString() {
-		return "ServiceKafkaClient [serviceName=" + serviceName
-				+ ", configs=" + configs + "]";
-	}
-
-	private Integer getIntProperty(String key, int defaultValue) {
-		if (key == null) {
-			return defaultValue;
-		}
-		String rtrnVal = configs.get(key);
-		if (rtrnVal == null) {
-			return defaultValue;
-		}
-		return Integer.valueOf(rtrnVal);
-	}
-
-	private String getJAASConfig(Map<String,String> configs){
-		String jaasConfig =  new StringBuilder()
-				.append(JAAS_KRB5_MODULE).append(" ")
-				.append(JAAS_USE_KEYTAB).append(" ")
-				.append(JAAS_KEYTAB).append(configs.get(KEY_KAFKA_KEYTAB)).append("\"").append(" ")
-				.append(JAAS_STOKE_KEY).append(" ")
-				.append(JAAS_USER_TICKET_CACHE).append(" ")
-				.append(JAAS_SERVICE_NAME).append(" ")
-				.append(JAAS_PRINCIPAL).append(configs.get(KEY_KAFKA_PRINCIPAL)).append("\";")
-				.toString();
-		if (LOG.isDebugEnabled()) {
-			LOG.debug("KafkaClient JAAS: " + jaasConfig);
-		}
-		return jaasConfig;
-	}
-
-}
diff --git a/plugin-kafka/src/main/java/org/apache/ranger/services/kafka/client/ServiceKafkaConnectionMgr.java b/plugin-kafka/src/main/java/org/apache/ranger/services/kafka/client/ServiceKafkaConnectionMgr.java
deleted file mode 100644
index 60c55cc13..000000000
--- a/plugin-kafka/src/main/java/org/apache/ranger/services/kafka/client/ServiceKafkaConnectionMgr.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.ranger.services.kafka.client;
-
-import org.apache.commons.lang.StringUtils;
-import org.apache.kafka.clients.admin.AdminClientConfig;
-import java.util.Map;
-
-public class ServiceKafkaConnectionMgr {
-	private static final String SEPARATOR			= ",";
-	private static final String KEY_SASL_MECHANISM	= "sasl.mechanism";
-	private static final String KEY_KAFKA_KEYTAB    = "kafka.keytab";
-	private static final String KEY_KAFKA_PRINCIPAL = "kafka.principal";
-
-	static public ServiceKafkaClient getKafkaClient(String serviceName,
-													Map<String, String> configs) throws Exception {
-		String error = getServiceConfigValidationErrors(configs);
-		if (StringUtils.isNotBlank(error)){
-			error =  "JAAS configuration missing or not correct in Ranger Kafka Service..." + error;
-			throw new Exception(error);
-		}
-		ServiceKafkaClient serviceKafkaClient = new ServiceKafkaClient(serviceName, configs);
-		return serviceKafkaClient;
-	}
-
-	/**
-	 * @param serviceName
-	 * @param configs
-	 * @return
-	 */
-	public static Map<String, Object> connectionTest(String serviceName,
-			Map<String, String> configs) throws Exception {
-		ServiceKafkaClient serviceKafkaClient = getKafkaClient(serviceName,
-				configs);
-		return serviceKafkaClient.connectionTest();
-	}
-
-	private static String  getServiceConfigValidationErrors(Map<String, String> configs) {
-		StringBuilder ret = new StringBuilder();
-
-		String bootstrap_servers = configs.get(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG);
-		String security_protocol = configs.get(AdminClientConfig.SECURITY_PROTOCOL_CONFIG);
-		String sasl_mechanism = configs.get(KEY_SASL_MECHANISM);
-		String kafka_keytab = configs.get(KEY_KAFKA_KEYTAB);
-		String kafka_principal = configs.get(KEY_KAFKA_PRINCIPAL);
-
-		if (StringUtils.isEmpty(bootstrap_servers)) {
-			ret.append(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG);
-		}
-
-		if (StringUtils.isEmpty(security_protocol)) {
-			if (StringUtils.isNotBlank(ret.toString())) {
-				ret.append(SEPARATOR).append(AdminClientConfig.SECURITY_PROTOCOL_CONFIG);
-			} else {
-				ret.append(AdminClientConfig.SECURITY_PROTOCOL_CONFIG);
-			}
-		}
-
-		if (StringUtils.isEmpty(sasl_mechanism)) {
-			if (StringUtils.isNotBlank(ret.toString())) {
-				ret.append(SEPARATOR).append(KEY_SASL_MECHANISM);
-			} else {
-				ret.append(KEY_SASL_MECHANISM);
-			}
-		}
-
-		if (StringUtils.isEmpty(kafka_keytab)) {
-			if (StringUtils.isNotBlank(ret.toString())) {
-				ret.append(SEPARATOR).append(KEY_KAFKA_KEYTAB);
-			} else {
-				ret.append(KEY_KAFKA_KEYTAB);
-			}
-		}
-
-		if (StringUtils.isEmpty(kafka_principal)) {
-			if (StringUtils.isNotBlank(ret.toString())) {
-				ret.append(SEPARATOR).append(KEY_KAFKA_PRINCIPAL);
-			} else {
-				ret.append(KEY_KAFKA_PRINCIPAL);
-			}
-		}
-		return ret.toString();
-	}
-}
diff --git a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerAuthorizerGSSTest.java b/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerAuthorizerGSSTest.java
deleted file mode 100644
index fe600b1ea..000000000
--- a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerAuthorizerGSSTest.java
+++ /dev/null
@@ -1,334 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.ranger.authorization.kafka.authorizer;
-
-import java.io.File;
-import java.net.ServerSocket;
-import java.nio.charset.StandardCharsets;
-import java.nio.file.FileSystems;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import java.util.concurrent.Future;
-
-import org.apache.curator.test.InstanceSpec;
-import org.apache.curator.test.TestingServer;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.kafka.clients.CommonClientConfigs;
-import org.apache.kafka.clients.consumer.ConsumerRecord;
-import org.apache.kafka.clients.consumer.ConsumerRecords;
-import org.apache.kafka.clients.consumer.KafkaConsumer;
-import org.apache.kafka.clients.producer.KafkaProducer;
-import org.apache.kafka.clients.producer.Producer;
-import org.apache.kafka.clients.producer.ProducerRecord;
-import org.apache.kafka.clients.producer.RecordMetadata;
-import org.apache.kafka.common.PartitionInfo;
-import org.apache.kafka.common.config.SaslConfigs;
-import org.apache.kerby.kerberos.kerb.server.SimpleKdcServer;
-import org.junit.Assert;
-import org.junit.Test;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import kafka.server.KafkaConfig;
-import kafka.server.KafkaServerStartable;
-
-/**
- * A simple test that starts a Kafka broker, creates "test" and "dev" topics,
- * sends a message to them and consumes it.
- * The RangerKafkaAuthorizer enforces the following authorization rules:
- *
- *  - The "IT" group can do anything
- *  - The "public" group can "read/describe/write" on the "test" topic.
- *
- * Policies available from admin via:
- *
- * http://localhost:6080/service/plugins/policies/download/cl1_kafka
- *
- * Authentication is done via Kerberos/GSS.
- */
-public class KafkaRangerAuthorizerGSSTest {
-    private final static Logger LOG = LoggerFactory.getLogger(KafkaRangerAuthorizerGSSTest.class);
-
-    private static KafkaServerStartable kafkaServer;
-    private static TestingServer zkServer;
-    private static int port;
-    private static Path tempDir;
-    private static SimpleKdcServer kerbyServer;
-
-    @org.junit.BeforeClass
-    public static void setup() throws Exception {
-        String basedir = System.getProperty("basedir");
-        if (basedir == null) {
-            basedir = new File(".").getCanonicalPath();
-        }
-
-        configureKerby(basedir);
-
-        // JAAS Config file - We need to point to the correct keytab files
-        Path path = FileSystems.getDefault().getPath(basedir, "/src/test/resources/kafka_kerberos.jaas");
-        String content = new String(Files.readAllBytes(path), StandardCharsets.UTF_8);
-        content = content.replaceAll("<basedir>", basedir);
-        //content = content.replaceAll("zookeeper/localhost", "zookeeper/" + address);
-
-        Path path2 = FileSystems.getDefault().getPath(basedir, "/target/test-classes/kafka_kerberos.jaas");
-        Files.write(path2, content.getBytes(StandardCharsets.UTF_8));
-
-        System.setProperty("java.security.auth.login.config", path2.toString());
-
-        // Set up Zookeeper to require SASL
-        Map<String,Object> zookeeperProperties = new HashMap<>();
-        zookeeperProperties.put("authProvider.1", "org.apache.zookeeper.server.auth.SASLAuthenticationProvider");
-        zookeeperProperties.put("requireClientAuthScheme", "sasl");
-        zookeeperProperties.put("jaasLoginRenew", "3600000");
-
-        InstanceSpec instanceSpec = new InstanceSpec(null, -1, -1, -1, true, 1,-1, -1, zookeeperProperties, "localhost");
-
-        zkServer = new TestingServer(instanceSpec, true);
-
-        // Get a random port
-        ServerSocket serverSocket = new ServerSocket(0);
-        port = serverSocket.getLocalPort();
-        serverSocket.close();
-
-        tempDir = Files.createTempDirectory("kafka");
-
-        LOG.info("Port is {}", port);
-        LOG.info("Temporary directory is at {}", tempDir);
-
-        final Properties props = new Properties();
-        props.put("broker.id", 1);
-        props.put("host.name", "localhost");
-        props.put("port", port);
-        props.put("log.dir", tempDir.toString());
-        props.put("zookeeper.connect", zkServer.getConnectString());
-        props.put("replica.socket.timeout.ms", "1500");
-        props.put("controlled.shutdown.enable", Boolean.TRUE.toString());
-        // Enable SASL_PLAINTEXT
-        props.put("listeners", "SASL_PLAINTEXT://localhost:" + port);
-        props.put("security.inter.broker.protocol", "SASL_PLAINTEXT");
-        props.put("sasl.enabled.mechanisms", "GSSAPI");
-        props.put("sasl.mechanism.inter.broker.protocol", "GSSAPI");
-        props.put("sasl.kerberos.service.name", "kafka");
-        props.put("offsets.topic.replication.factor", (short) 1);
-        props.put("offsets.topic.num.partitions", 1);
-
-        // Plug in Apache Ranger authorizer
-        props.put("authorizer.class.name", "org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer");
-
-        // Create users for testing
-        UserGroupInformation.createUserForTesting("client@kafka.apache.org", new String[] {"public"});
-        UserGroupInformation.createUserForTesting("kafka/localhost@kafka.apache.org", new String[] {"IT"});
-
-        KafkaConfig config = new KafkaConfig(props);
-        kafkaServer = new KafkaServerStartable(config);
-        kafkaServer.startup();
-
-        // Create some topics
-        final Properties adminProps = new Properties();
-        adminProps.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "localhost:" + port);
-        adminProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT");
-        adminProps.put(SaslConfigs.SASL_MECHANISM, "GSSAPI");
-        KafkaTestUtils.createSomeTopics(adminProps);
-    }
-
-    private static void configureKerby(String baseDir) throws Exception {
-
-        //System.setProperty("sun.security.krb5.debug", "true");
-        System.setProperty("java.security.krb5.conf", baseDir + "/target/krb5.conf");
-
-        kerbyServer = new SimpleKdcServer();
-
-        kerbyServer.setKdcRealm("kafka.apache.org");
-        kerbyServer.setAllowUdp(false);
-        kerbyServer.setWorkDir(new File(baseDir + "/target"));
-
-        kerbyServer.init();
-
-        // Create principals
-        String zookeeper = "zookeeper/localhost@kafka.apache.org";
-        String kafka = "kafka/localhost@kafka.apache.org";
-        String client = "client@kafka.apache.org";
-
-        kerbyServer.createPrincipal(zookeeper, "zookeeper");
-        File keytabFile = new File(baseDir + "/target/zookeeper.keytab");
-        kerbyServer.exportPrincipal(zookeeper, keytabFile);
-
-        kerbyServer.createPrincipal(kafka, "kafka");
-        keytabFile = new File(baseDir + "/target/kafka.keytab");
-        kerbyServer.exportPrincipal(kafka, keytabFile);
-
-        kerbyServer.createPrincipal(client, "client");
-        keytabFile = new File(baseDir + "/target/client.keytab");
-        kerbyServer.exportPrincipal(client, keytabFile);
-
-        kerbyServer.start();
-    }
-
-    @org.junit.AfterClass
-    public static void cleanup() throws Exception {
-        if (kafkaServer != null) {
-            kafkaServer.shutdown();
-        }
-        if (zkServer != null) {
-            zkServer.stop();
-        }
-        if (kerbyServer != null) {
-            kerbyServer.stop();
-        }
-    }
-
-    // The "public" group can write to and read from "test"
-    @Test
-    public void testAuthorizedRead() {
-        // Create the Producer
-        Properties producerProps = new Properties();
-        producerProps.put("bootstrap.servers", "localhost:" + port);
-        producerProps.put("acks", "all");
-        producerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT");
-        producerProps.put("sasl.mechanism", "GSSAPI");
-        producerProps.put("sasl.kerberos.service.name", "kafka");
-
-        final Producer<String, String> producer = new KafkaProducer<>(producerProps);
-
-        // Create the Consumer
-        Properties consumerProps = new Properties();
-        consumerProps.put("bootstrap.servers", "localhost:" + port);
-        consumerProps.put("group.id", "consumerTestGroup");
-        consumerProps.put("enable.auto.commit", "true");
-        consumerProps.put("auto.offset.reset", "earliest");
-        consumerProps.put("auto.commit.interval.ms", "1000");
-        consumerProps.put("session.timeout.ms", "30000");
-        consumerProps.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
-        consumerProps.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
-        consumerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT");
-        consumerProps.put("sasl.mechanism", "GSSAPI");
-        consumerProps.put("sasl.kerberos.service.name", "kafka");
-
-        final KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps);
-        checkTopicExists(consumer);
-        LOG.info("Subscribing to 'test'");
-        consumer.subscribe(Arrays.asList("test"));
-
-        sendMessage(producer);
-
-        // Poll until we consume it
-        ConsumerRecord<String, String> record = null;
-        for (int i = 0; i < 1000; i++) {
-            LOG.info("Waiting for messages {}. try", i);
-            ConsumerRecords<String, String> records = consumer.poll(100);
-            if (records.count() > 0) {
-                LOG.info("Found {} messages", records.count());
-                record = records.iterator().next();
-                break;
-            }
-            sleep();
-        }
-
-        Assert.assertNotNull(record);
-        Assert.assertEquals("somevalue", record.value());
-
-        producer.close();
-        consumer.close();
-    }
-
-    private void checkTopicExists(final KafkaConsumer<String, String> consumer) {
-        Map<String, List<PartitionInfo>> topics = consumer.listTopics();
-        while (!topics.containsKey("test")) {
-            LOG.warn("Required topic is not available, only {} present", topics.keySet());
-            sleep();
-            topics = consumer.listTopics();
-        }
-        LOG.warn("Available topics: {}", topics.keySet());
-    }
-
-    private void sendMessage(final Producer<String, String> producer) {
-        // Send a message
-        try {
-            LOG.info("Send a message to 'test'");
-            producer.send(new ProducerRecord<String, String>("test", "somekey", "somevalue"));
-            producer.flush();
-        } catch (RuntimeException e) {
-            LOG.error("Unable to send message to topic 'test' ", e);
-        }
-    }
-
-    private void sleep() {
-        try {
-            Thread.sleep(1000);
-        } catch (InterruptedException e) {
-            LOG.info("Interrupted sleep, nothing important");
-        }
-    }
-
-    // The "public" group can't write to "dev"
-    @Test
-    public void testUnauthorizedWrite() throws Exception {
-        // Create the Producer
-        Properties producerProps = new Properties();
-        producerProps.put("bootstrap.servers", "localhost:" + port);
-        producerProps.put("acks", "all");
-        producerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT");
-        producerProps.put("sasl.mechanism", "GSSAPI");
-        producerProps.put("sasl.kerberos.service.name", "kafka");
-
-        final Producer<String, String> producer = new KafkaProducer<>(producerProps);
-
-        // Send a message
-        try {
-            Future<RecordMetadata> record =
-                producer.send(new ProducerRecord<String, String>("dev", "somekey", "somevalue"));
-            producer.flush();
-            record.get();
-        } catch (Exception ex) {
-            Assert.assertTrue(ex.getMessage().contains("Not authorized to access topics"));
-        }
-
-        producer.close();
-    }
-
-
-    @Test
-    public void testAuthorizedIdempotentWrite() throws Exception {
-        // Create the Producer
-        Properties producerProps = new Properties();
-        producerProps.put("bootstrap.servers", "localhost:" + port);
-        producerProps.put("acks", "all");
-        producerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT");
-        producerProps.put("sasl.mechanism", "GSSAPI");
-        producerProps.put("sasl.kerberos.service.name", "kafka");
-        producerProps.put("enable.idempotence", "true");
-
-        final Producer<String, String> producer = new KafkaProducer<>(producerProps);
-
-        // Send a message
-        producer.send(new ProducerRecord<String, String>("test", "somekey", "somevalue"));
-        producer.flush();
-        producer.close();
-    }
-}
diff --git a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerAuthorizerSASLSSLTest.java b/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerAuthorizerSASLSSLTest.java
deleted file mode 100644
index 6f4538549..000000000
--- a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerAuthorizerSASLSSLTest.java
+++ /dev/null
@@ -1,277 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.ranger.authorization.kafka.authorizer;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.OutputStream;
-import java.math.BigInteger;
-import java.net.ServerSocket;
-import java.security.KeyStore;
-import java.util.Arrays;
-import java.util.Properties;
-import java.util.concurrent.Future;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.curator.test.TestingServer;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.kafka.clients.CommonClientConfigs;
-import org.apache.kafka.clients.consumer.ConsumerRecord;
-import org.apache.kafka.clients.consumer.ConsumerRecords;
-import org.apache.kafka.clients.consumer.KafkaConsumer;
-import org.apache.kafka.clients.producer.KafkaProducer;
-import org.apache.kafka.clients.producer.Producer;
-import org.apache.kafka.clients.producer.ProducerRecord;
-import org.apache.kafka.clients.producer.RecordMetadata;
-import org.apache.kafka.common.config.SaslConfigs;
-import org.apache.kafka.common.config.SslConfigs;
-import org.junit.Assert;
-import org.junit.Test;
-
-import kafka.server.KafkaConfig;
-import kafka.server.KafkaServerStartable;
-
-/**
- * A simple test that starts a Kafka broker, creates "test" and "dev" topics, sends a message to them and consumes it. We also plug in a 
- * CustomAuthorizer that enforces some authorization rules:
- * 
- *  - The "IT" group can do anything
- *  - The "public" group can "read/describe/write" on the "test" topic.
- *  - The "public" group can only "read/describe" on the "dev" topic, but not write.
- * 
- * Policies available from admin via:
- * 
- * http://localhost:6080/service/plugins/policies/download/cl1_kafka
- * 
- * Clients and services authenticate to Kafka using the SASL SSL protocol as part of this test.
- */
-@org.junit.Ignore("Causing JVM to abort on some platforms")
-public class KafkaRangerAuthorizerSASLSSLTest {
-    
-    private static KafkaServerStartable kafkaServer;
-    private static TestingServer zkServer;
-    private static int port;
-    private static String serviceKeystorePath;
-    private static String clientKeystorePath;
-    private static String truststorePath;
-    
-    @org.junit.BeforeClass
-    public static void setup() throws Exception {
-    	// JAAS Config file
-        String basedir = System.getProperty("basedir");
-        if (basedir == null) {
-            basedir = new File(".").getCanonicalPath();
-        }
-
-        File f = new File(basedir + "/src/test/resources/kafka_plain.jaas");
-        System.setProperty("java.security.auth.login.config", f.getPath());
-        
-    	// Create keys
-    	String serviceDN = "CN=Service,O=Apache,L=Dublin,ST=Leinster,C=IE";
-    	String clientDN = "CN=Client,O=Apache,L=Dublin,ST=Leinster,C=IE";
-    	
-    	// Create a truststore
-    	KeyStore keystore = KeyStore.getInstance(KeyStore.getDefaultType());
-    	keystore.load(null, "security".toCharArray());
-    	
-    	serviceKeystorePath = 
-    			KafkaTestUtils.createAndStoreKey(serviceDN, serviceDN, BigInteger.valueOf(30), 
-    					"sspass", "myservicekey", "skpass", keystore);
-    	clientKeystorePath = 
-    			KafkaTestUtils.createAndStoreKey(clientDN, clientDN, BigInteger.valueOf(31), 
-    					"cspass", "myclientkey", "ckpass", keystore);
-    	
-    	File truststoreFile = File.createTempFile("kafkatruststore", ".jks");
-    	try (OutputStream output = new FileOutputStream(truststoreFile)) {
-    		keystore.store(output, "security".toCharArray());
-    	}
-    	truststorePath = truststoreFile.getPath();
-    			
-        zkServer = new TestingServer();
-        
-        // Get a random port
-        ServerSocket serverSocket = new ServerSocket(0);
-        port = serverSocket.getLocalPort();
-        serverSocket.close();
-        
-        final Properties props = new Properties();
-        props.put("broker.id", 1);
-        props.put("host.name", "localhost");
-        props.put("port", port);
-        props.put("log.dir", "/tmp/kafka");
-        props.put("zookeeper.connect", zkServer.getConnectString());
-        props.put("replica.socket.timeout.ms", "1500");
-        props.put("controlled.shutdown.enable", Boolean.TRUE.toString());
-        // Enable SASL_SSL
-        props.put("listeners", "SASL_SSL://localhost:" + port);
-        props.put("security.inter.broker.protocol", "SASL_SSL");
-        props.put("sasl.enabled.mechanisms", "PLAIN");
-        props.put("sasl.mechanism.inter.broker.protocol", "PLAIN");
-
-        props.put("offsets.topic.replication.factor", (short) 1);
-        props.put("offsets.topic.num.partitions", 1);
-
-        props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, serviceKeystorePath);
-        props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "sspass");
-        props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "skpass");
-        props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-        
-        // Plug in Apache Ranger authorizer
-        props.put("authorizer.class.name", "org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer");
-        
-        // Create users for testing
-        UserGroupInformation.createUserForTesting("alice", new String[] {"IT"});
-        
-        KafkaConfig config = new KafkaConfig(props);
-        kafkaServer = new KafkaServerStartable(config);
-        kafkaServer.startup();
-
-        // Create some topics
-        final Properties adminProps = new Properties();
-        adminProps.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "localhost:" + port);
-        adminProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL");
-        adminProps.put(SaslConfigs.SASL_MECHANISM, "PLAIN");
-        // ssl
-        adminProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, serviceKeystorePath);
-        adminProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "sspass");
-        adminProps.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "skpass");
-        adminProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        adminProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-        KafkaTestUtils.createSomeTopics(adminProps);
-    }
-    
-    @org.junit.AfterClass
-    public static void cleanup() throws Exception {
-        if (kafkaServer != null) {
-            kafkaServer.shutdown();
-        }
-        if (zkServer != null) {
-            zkServer.stop();
-        }
-        
-        File clientKeystoreFile = new File(clientKeystorePath);
-        if (clientKeystoreFile.exists()) {
-        	FileUtils.forceDelete(clientKeystoreFile);
-        }
-        File serviceKeystoreFile = new File(serviceKeystorePath);
-        if (serviceKeystoreFile.exists()) {
-        	FileUtils.forceDelete(serviceKeystoreFile);
-        }
-        File truststoreFile = new File(truststorePath);
-        if (truststoreFile.exists()) {
-        	FileUtils.forceDelete(truststoreFile);
-        }
-    }
-    
-    @Test
-    public void testAuthorizedRead() throws Exception {
-        // Create the Producer
-        Properties producerProps = new Properties();
-        producerProps.put("bootstrap.servers", "localhost:" + port);
-        producerProps.put("acks", "all");
-        producerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL");
-        producerProps.put("sasl.mechanism", "PLAIN");
-        
-        producerProps.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "JKS");
-        producerProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, serviceKeystorePath);
-        producerProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "sspass");
-        producerProps.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "skpass");
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-        
-        final Producer<String, String> producer = new KafkaProducer<>(producerProps);
-        
-        // Create the Consumer
-        Properties consumerProps = new Properties();
-        consumerProps.put("bootstrap.servers", "localhost:" + port);
-        consumerProps.put("group.id", "test");
-        consumerProps.put("enable.auto.commit", "true");
-        consumerProps.put("auto.offset.reset", "earliest");
-        consumerProps.put("auto.commit.interval.ms", "1000");
-        consumerProps.put("session.timeout.ms", "30000");
-        consumerProps.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
-        consumerProps.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
-        consumerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL");
-        consumerProps.put("sasl.mechanism", "PLAIN");
-        
-        consumerProps.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "JKS");
-        consumerProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, clientKeystorePath);
-        consumerProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "cspass");
-        consumerProps.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "ckpass");
-        consumerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        consumerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-        
-        final KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps);
-        consumer.subscribe(Arrays.asList("test"));
-        
-        // Send a message
-        producer.send(new ProducerRecord<String, String>("test", "somekey", "somevalue"));
-        producer.flush();
-        
-        // Poll until we consume it
-        
-        ConsumerRecord<String, String> record = null;
-        for (int i = 0; i < 1000; i++) {
-            ConsumerRecords<String, String> records = consumer.poll(100);
-            if (records.count() > 0) {
-                record = records.iterator().next();
-                break;
-            }
-            Thread.sleep(1000);
-        }
-
-        Assert.assertNotNull(record);
-        Assert.assertEquals("somevalue", record.value());
-
-        producer.close();
-        consumer.close();
-    }
-    
-    @Test
-    public void testAuthorizedWrite() throws Exception {
-        // Create the Producer
-        Properties producerProps = new Properties();
-        producerProps.put("bootstrap.servers", "localhost:" + port);
-        producerProps.put("acks", "all");
-        producerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL");
-        producerProps.put("sasl.mechanism", "PLAIN");
-        
-        producerProps.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "JKS");
-        producerProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, serviceKeystorePath);
-        producerProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "sspass");
-        producerProps.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "skpass");
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-        
-        final Producer<String, String> producer = new KafkaProducer<>(producerProps);
-        
-        // Send a message
-        Future<RecordMetadata> record = 
-            producer.send(new ProducerRecord<String, String>("dev", "somekey", "somevalue"));
-        producer.flush();
-        record.get();
-
-        producer.close();
-    }
-    
-}
diff --git a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerAuthorizerTest.java b/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerAuthorizerTest.java
deleted file mode 100644
index 113ac3686..000000000
--- a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerAuthorizerTest.java
+++ /dev/null
@@ -1,367 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.ranger.authorization.kafka.authorizer;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.OutputStream;
-import java.math.BigInteger;
-import java.net.ServerSocket;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.security.KeyStore;
-import java.util.Arrays;
-import java.util.Properties;
-import java.util.concurrent.Future;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.curator.test.TestingServer;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.kafka.clients.CommonClientConfigs;
-import org.apache.kafka.clients.consumer.ConsumerRecord;
-import org.apache.kafka.clients.consumer.ConsumerRecords;
-import org.apache.kafka.clients.consumer.KafkaConsumer;
-import org.apache.kafka.clients.producer.KafkaProducer;
-import org.apache.kafka.clients.producer.Producer;
-import org.apache.kafka.clients.producer.ProducerRecord;
-import org.apache.kafka.clients.producer.RecordMetadata;
-import org.apache.kafka.common.config.SslConfigs;
-import org.junit.Assert;
-import org.junit.Test;
-
-import kafka.server.KafkaConfig;
-import kafka.server.KafkaServerStartable;
-
-/**
- * A simple test that starts a Kafka broker, creates "test" and "dev" topics, sends a message to them and consumes it. We also plug in a 
- * CustomAuthorizer that enforces some authorization rules:
- * 
- *  - The "IT" group can do anything
- *  - The "public" group can "read/describe/write" on the "test" topic.
- *  - The "public" group can only "read/describe" on the "dev" topic, but not write.
- *
- * In addition we have a TAG based policy, which grants "read/describe" access to the "public" group to the "messages" topic (which is associated
- * with the tag called "MessagesTag". A "kafka_topic" entity was created in Apache Atlas + then associated with the "MessagesTag". This was
- * then imported into Ranger using the TagSyncService. The policies were then downloaded locally and saved for testing off-line.
- * 
- * Policies available from admin via:
- * 
- * http://localhost:6080/service/plugins/policies/download/cl1_kafka
- */
-public class KafkaRangerAuthorizerTest {
-    
-    private static KafkaServerStartable kafkaServer;
-    private static TestingServer zkServer;
-    private static int port;
-    private static String serviceKeystorePath;
-    private static String clientKeystorePath;
-    private static String truststorePath;
-    private static Path tempDir;
-    
-    @org.junit.BeforeClass
-    public static void setup() throws Exception {
-    	// Create keys
-        String serviceDN = "CN=localhost,O=Apache,L=Dublin,ST=Leinster,C=IE";
-        String clientDN = "CN=localhost,O=Apache,L=Dublin,ST=Leinster,C=IE";
-    	
-    	// Create a truststore
-    	KeyStore keystore = KeyStore.getInstance(KeyStore.getDefaultType());
-    	keystore.load(null, "security".toCharArray());
-    	
-    	serviceKeystorePath = 
-    			KafkaTestUtils.createAndStoreKey(serviceDN, serviceDN, BigInteger.valueOf(30), 
-    					"sspass", "myservicekey", "skpass", keystore);
-    	clientKeystorePath = 
-    			KafkaTestUtils.createAndStoreKey(clientDN, clientDN, BigInteger.valueOf(31), 
-    					"cspass", "myclientkey", "ckpass", keystore);
-    	
-    	File truststoreFile = File.createTempFile("kafkatruststore", ".jks");
-    	try (OutputStream output = new FileOutputStream(truststoreFile)) {
-    		keystore.store(output, "security".toCharArray());
-    	}
-    	truststorePath = truststoreFile.getPath();
-    			
-        zkServer = new TestingServer();
-        
-        // Get a random port
-        ServerSocket serverSocket = new ServerSocket(0);
-        port = serverSocket.getLocalPort();
-        serverSocket.close();
-
-        tempDir = Files.createTempDirectory("kafka");
-        
-        final Properties props = new Properties();
-        props.put("broker.id", 1);
-        props.put("host.name", "localhost");
-        props.put("port", port);
-        props.put("log.dir", tempDir.toString());
-        props.put("zookeeper.connect", zkServer.getConnectString());
-        props.put("replica.socket.timeout.ms", "1500");
-        props.put("controlled.shutdown.enable", Boolean.TRUE.toString());
-        // Enable SSL
-        props.put("listeners", "SSL://localhost:" + port);
-        props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, serviceKeystorePath);
-        props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "sspass");
-        props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "skpass");
-        props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-        props.put("security.inter.broker.protocol", "SSL");
-        props.put("ssl.client.auth", "required");
-        props.put("offsets.topic.replication.factor", (short) 1);
-        props.put("offsets.topic.num.partitions", 1);
-
-        // Plug in Apache Ranger authorizer
-        props.put("authorizer.class.name", "org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer");
-        
-        // Create users for testing
-        UserGroupInformation.createUserForTesting(clientDN, new String[] {"public"});
-        UserGroupInformation.createUserForTesting(serviceDN, new String[] {"IT"});
-        
-        KafkaConfig config = new KafkaConfig(props);
-        kafkaServer = new KafkaServerStartable(config);
-        kafkaServer.startup();
-
-        // Create some topics
-        final Properties adminProps = new Properties();
-        adminProps.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "localhost:" + port);
-        adminProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SSL");
-        // ssl
-        adminProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, serviceKeystorePath);
-        adminProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "sspass");
-        adminProps.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "skpass");
-        adminProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        adminProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-        KafkaTestUtils.createSomeTopics(adminProps);
-    }
-    
-    @org.junit.AfterClass
-    public static void cleanup() throws Exception {
-        if (kafkaServer != null) {
-            kafkaServer.shutdown();
-        }
-        if (zkServer != null) {
-            zkServer.stop();
-        }
-        
-        File clientKeystoreFile = new File(clientKeystorePath);
-        if (clientKeystoreFile.exists()) {
-        	FileUtils.forceDelete(clientKeystoreFile);
-        }
-        File serviceKeystoreFile = new File(serviceKeystorePath);
-        if (serviceKeystoreFile.exists()) {
-        	FileUtils.forceDelete(serviceKeystoreFile);
-        }
-        File truststoreFile = new File(truststorePath);
-        if (truststoreFile.exists()) {
-        	FileUtils.forceDelete(truststoreFile);
-        }
-        if (tempDir != null) {
-            FileUtils.deleteDirectory(tempDir.toFile());
-        }
-    }
-    
-    // The "public" group can read from "test"
-    @Test
-    public void testAuthorizedRead() throws Exception {
-        // Create the Producer
-        Properties producerProps = new Properties();
-        producerProps.put("bootstrap.servers", "localhost:" + port);
-        producerProps.put("acks", "all");
-        producerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SSL");
-        producerProps.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "JKS");
-        producerProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, serviceKeystorePath);
-        producerProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "sspass");
-        producerProps.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "skpass");
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-        
-        final Producer<String, String> producer = new KafkaProducer<>(producerProps);
-        
-        // Create the Consumer
-        Properties consumerProps = new Properties();
-        consumerProps.put("bootstrap.servers", "localhost:" + port);
-        consumerProps.put("group.id", "test");
-        consumerProps.put("enable.auto.commit", "true");
-        consumerProps.put("auto.offset.reset", "earliest");
-        consumerProps.put("auto.commit.interval.ms", "1000");
-        consumerProps.put("session.timeout.ms", "30000");
-        consumerProps.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
-        consumerProps.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
-        consumerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SSL");
-        consumerProps.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "JKS");
-        consumerProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, clientKeystorePath);
-        consumerProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "cspass");
-        consumerProps.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "ckpass");
-        consumerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        consumerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-        
-        final KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps);
-        consumer.subscribe(Arrays.asList("test"));
-        
-        // Send a message
-        producer.send(new ProducerRecord<String, String>("test", "somekey", "somevalue"));
-        producer.flush();
-        
-        // Poll until we consume it
-        
-        ConsumerRecord<String, String> record = null;
-        for (int i = 0; i < 1000; i++) {
-            ConsumerRecords<String, String> records = consumer.poll(100);
-            if (records.count() > 0) {
-                record = records.iterator().next();
-                break;
-            }
-            Thread.sleep(1000);
-        }
-
-        Assert.assertNotNull(record);
-        Assert.assertEquals("somevalue", record.value());
-
-        producer.close();
-        consumer.close();
-    }
-    
-    // The "IT" group can write to any topic
-    @Test
-    public void testAuthorizedWrite() throws Exception {
-        // Create the Producer
-        Properties producerProps = new Properties();
-        producerProps.put("bootstrap.servers", "localhost:" + port);
-        producerProps.put("acks", "all");
-        producerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SSL");
-        producerProps.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "JKS");
-        producerProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, serviceKeystorePath);
-        producerProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "sspass");
-        producerProps.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "skpass");
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-        
-        final Producer<String, String> producer = new KafkaProducer<>(producerProps);
-        // Send a message
-        Future<RecordMetadata> record = 
-            producer.send(new ProducerRecord<String, String>("dev", "somekey", "somevalue"));
-        producer.flush();
-        record.get();
-
-        producer.close();
-    }
-    
-    // The "public" group can write to "test" but not "dev"
-    @Test
-    public void testUnauthorizedWrite() throws Exception {
-        // Create the Producer
-        Properties producerProps = new Properties();
-        producerProps.put("bootstrap.servers", "localhost:" + port);
-        producerProps.put("acks", "all");
-        producerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SSL");
-        producerProps.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "JKS");
-        producerProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, clientKeystorePath);
-        producerProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "cspass");
-        producerProps.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "ckpass");
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-        
-        final Producer<String, String> producer = new KafkaProducer<>(producerProps);
-        
-        // Send a message
-        Future<RecordMetadata> record =
-            producer.send(new ProducerRecord<String, String>("test", "somekey", "somevalue"));
-        producer.flush();
-        record.get();
-        
-        try {
-            record = producer.send(new ProducerRecord<String, String>("dev", "somekey", "somevalue"));
-            producer.flush();
-            record.get();
-        } catch (Exception ex) {
-            Assert.assertTrue(ex.getMessage().contains("Not authorized to access topics"));
-        }
-        
-        producer.close();
-    }
-
-    // The "public" group can read from "messages"
-    @Test
-    public void testAuthorizedReadUsingTagPolicy() throws Exception {
-        // Create the Producer
-        Properties producerProps = new Properties();
-        producerProps.put("bootstrap.servers", "localhost:" + port);
-        producerProps.put("acks", "all");
-        producerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
-        producerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SSL");
-        producerProps.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "JKS");
-        producerProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, serviceKeystorePath);
-        producerProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "sspass");
-        producerProps.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "skpass");
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        producerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-
-        final Producer<String, String> producer = new KafkaProducer<>(producerProps);
-
-        // Create the Consumer
-        Properties consumerProps = new Properties();
-        consumerProps.put("bootstrap.servers", "localhost:" + port);
-        consumerProps.put("group.id", "test");
-        consumerProps.put("enable.auto.commit", "true");
-        consumerProps.put("auto.offset.reset", "earliest");
-        consumerProps.put("auto.commit.interval.ms", "1000");
-        consumerProps.put("session.timeout.ms", "30000");
-        consumerProps.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
-        consumerProps.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
-        consumerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SSL");
-        consumerProps.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "JKS");
-        consumerProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, clientKeystorePath);
-        consumerProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "cspass");
-        consumerProps.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "ckpass");
-        consumerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, truststorePath);
-        consumerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "security");
-
-        final KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps);
-        consumer.subscribe(Arrays.asList("messages"));
-
-        // Send a message
-        producer.send(new ProducerRecord<String, String>("messages", "somekey", "somevalue"));
-        producer.flush();
-
-        // Poll until we consume it
-
-        ConsumerRecord<String, String> record = null;
-        for (int i = 0; i < 1000; i++) {
-            ConsumerRecords<String, String> records = consumer.poll(100);
-            if (records.count() > 0) {
-                record = records.iterator().next();
-                break;
-            }
-            Thread.sleep(1000);
-        }
-
-        Assert.assertNotNull(record);
-        Assert.assertEquals("somevalue", record.value());
-
-        producer.close();
-        consumer.close();
-    }
-
-}
diff --git a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerTopicCreationTest.java b/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerTopicCreationTest.java
deleted file mode 100644
index a12817eba..000000000
--- a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaRangerTopicCreationTest.java
+++ /dev/null
@@ -1,191 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.ranger.authorization.kafka.authorizer;
-
-import kafka.server.KafkaConfig;
-import kafka.server.KafkaServerStartable;
-import org.apache.curator.test.InstanceSpec;
-import org.apache.curator.test.TestingServer;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.kafka.clients.CommonClientConfigs;
-import org.apache.kafka.clients.admin.AdminClient;
-import org.apache.kafka.clients.admin.KafkaAdminClient;
-import org.apache.kafka.clients.admin.AdminClientConfig;
-import org.apache.kafka.clients.admin.CreateTopicsResult;
-import org.apache.kafka.clients.admin.NewTopic;
-import org.apache.kafka.common.KafkaFuture;
-import org.apache.kerby.kerberos.kerb.server.SimpleKdcServer;
-import org.junit.Test;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-import java.net.ServerSocket;
-import java.nio.charset.StandardCharsets;
-import java.nio.file.FileSystems;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Properties;
-
-
-public class KafkaRangerTopicCreationTest {
-    private final static Logger LOG = LoggerFactory.getLogger(KafkaRangerTopicCreationTest.class);
-
-    private static KafkaServerStartable kafkaServer;
-    private static TestingServer zkServer;
-    private static int port;
-    private static Path tempDir;
-    private static SimpleKdcServer kerbyServer;
-
-    @org.junit.BeforeClass
-    public static void setup() throws Exception {
-        String basedir = System.getProperty("basedir");
-        if (basedir == null) {
-            basedir = new File(".").getCanonicalPath();
-        }
-        System.out.println("Base Dir " + basedir);
-
-        configureKerby(basedir);
-
-        // JAAS Config file - We need to point to the correct keytab files
-        Path path = FileSystems.getDefault().getPath(basedir, "/src/test/resources/kafka_kerberos.jaas");
-        String content = new String(Files.readAllBytes(path), StandardCharsets.UTF_8);
-        content = content.replaceAll("<basedir>", basedir);
-        //content = content.replaceAll("zookeeper/localhost", "zookeeper/" + address);
-
-        Path path2 = FileSystems.getDefault().getPath(basedir, "/target/test-classes/kafka_kerberos.jaas");
-        Files.write(path2, content.getBytes(StandardCharsets.UTF_8));
-
-        System.setProperty("java.security.auth.login.config", path2.toString());
-
-        // Set up Zookeeper to require SASL
-        Map<String,Object> zookeeperProperties = new HashMap<>();
-        zookeeperProperties.put("authProvider.1", "org.apache.zookeeper.server.auth.SASLAuthenticationProvider");
-        zookeeperProperties.put("requireClientAuthScheme", "sasl");
-        zookeeperProperties.put("jaasLoginRenew", "3600000");
-
-        InstanceSpec instanceSpec = new InstanceSpec(null, -1, -1, -1, true, 1,-1, -1, zookeeperProperties, "localhost");
-
-        zkServer = new TestingServer(instanceSpec, true);
-
-        // Get a random port
-        ServerSocket serverSocket = new ServerSocket(0);
-        port = serverSocket.getLocalPort();
-        serverSocket.close();
-
-        tempDir = Files.createTempDirectory("kafka");
-
-        LOG.info("Port is {}", port);
-        LOG.info("Temporary directory is at {}", tempDir);
-
-        final Properties props = new Properties();
-        props.put("broker.id", 1);
-        props.put("host.name", "localhost");
-        props.put("port", port);
-        props.put("log.dir", tempDir.toString());
-        props.put("zookeeper.connect", zkServer.getConnectString());
-        props.put("replica.socket.timeout.ms", "1500");
-        props.put("controlled.shutdown.enable", Boolean.TRUE.toString());
-        // Enable SASL_PLAINTEXT
-        props.put("listeners", "SASL_PLAINTEXT://localhost:" + port);
-        props.put("security.inter.broker.protocol", "SASL_PLAINTEXT");
-        props.put("sasl.enabled.mechanisms", "GSSAPI");
-        props.put("sasl.mechanism.inter.broker.protocol", "GSSAPI");
-        props.put("sasl.kerberos.service.name", "kafka");
-        props.put("offsets.topic.replication.factor", (short) 1);
-        props.put("offsets.topic.num.partitions", 1);
-
-        // Plug in Apache Ranger authorizer
-        props.put("authorizer.class.name", "org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer");
-
-        // Create users for testing
-        UserGroupInformation.createUserForTesting("client@kafka.apache.org", new String[] {"public"});
-        UserGroupInformation.createUserForTesting("kafka/localhost@kafka.apache.org", new String[] {"IT"});
-
-        KafkaConfig config = new KafkaConfig(props);
-        kafkaServer = new KafkaServerStartable(config);
-        kafkaServer.startup();
-   }
-
-    private static void configureKerby(String baseDir) throws Exception {
-
-        //System.setProperty("sun.security.krb5.debug", "true");
-        System.setProperty("java.security.krb5.conf", baseDir + "/target/krb5.conf");
-
-        kerbyServer = new SimpleKdcServer();
-
-        kerbyServer.setKdcRealm("kafka.apache.org");
-        kerbyServer.setAllowUdp(false);
-        kerbyServer.setWorkDir(new File(baseDir + "/target"));
-
-        kerbyServer.init();
-
-        // Create principals
-        String zookeeper = "zookeeper/localhost@kafka.apache.org";
-        String kafka = "kafka/localhost@kafka.apache.org";
-        String client = "client@kafka.apache.org";
-
-        kerbyServer.createPrincipal(zookeeper, "zookeeper");
-        File keytabFile = new File(baseDir + "/target/zookeeper.keytab");
-        kerbyServer.exportPrincipal(zookeeper, keytabFile);
-
-        kerbyServer.createPrincipal(kafka, "kafka");
-        keytabFile = new File(baseDir + "/target/kafka.keytab");
-        kerbyServer.exportPrincipal(kafka, keytabFile);
-
-        kerbyServer.createPrincipal(client, "client");
-        keytabFile = new File(baseDir + "/target/client.keytab");
-        kerbyServer.exportPrincipal(client, keytabFile);
-
-        kerbyServer.start();
-    }
-
-    @org.junit.AfterClass
-    public static void cleanup() throws Exception {
-        if (kafkaServer != null) {
-            kafkaServer.shutdown();
-        }
-        if (zkServer != null) {
-            zkServer.stop();
-        }
-        if (kerbyServer != null) {
-            kerbyServer.stop();
-        }
-    }
-
-    @Test
-    public void testCreateTopic() throws Exception {
-            final String topic = "test";
-            Properties properties = new Properties();
-            properties.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:" + port);
-            properties.put("client.id", "test-consumer-id");
-            properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT");
-            AdminClient client = KafkaAdminClient.create(properties);
-            CreateTopicsResult result = client.createTopics(Arrays.asList(new NewTopic(topic, 1, (short) 1)));
-            result.values().get(topic).get();
-            for (Map.Entry<String, KafkaFuture<Void>> entry : result.values().entrySet()) {
-                System.out.println("Create Topic : " + entry.getKey() + " " +
-                        "isCancelled : " + entry.getValue().isCancelled() + " " +
-                        "isCompletedExceptionally : " + entry.getValue().isCompletedExceptionally() + " " +
-                        "isDone : " + entry.getValue().isDone());
-            }
-    }
-}
diff --git a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaTestUtils.java b/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaTestUtils.java
deleted file mode 100644
index dc8277051..000000000
--- a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/KafkaTestUtils.java
+++ /dev/null
@@ -1,89 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.ranger.authorization.kafka.authorizer;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.OutputStream;
-import java.math.BigInteger;
-import java.security.KeyPair;
-import java.security.KeyPairGenerator;
-import java.security.KeyStore;
-import java.security.SecureRandom;
-import java.security.cert.Certificate;
-import java.security.cert.X509Certificate;
-import java.util.Arrays;
-import java.util.Date;
-import java.util.Properties;
-
-import org.apache.kafka.clients.admin.AdminClient;
-import org.apache.kafka.clients.admin.NewTopic;
-import org.bouncycastle.asn1.x500.X500Name;
-import org.bouncycastle.asn1.x500.style.RFC4519Style;
-import org.bouncycastle.asn1.x509.SubjectPublicKeyInfo;
-import org.bouncycastle.cert.X509v3CertificateBuilder;
-import org.bouncycastle.cert.jcajce.JcaX509CertificateConverter;
-import org.bouncycastle.operator.ContentSigner;
-import org.bouncycastle.operator.jcajce.JcaContentSignerBuilder;
-
-public final class KafkaTestUtils {
-    
-    public static String createAndStoreKey(String subjectName, String issuerName, BigInteger serial, String keystorePassword,
-    		String keystoreAlias, String keyPassword, KeyStore trustStore) throws Exception {
-    	
-    	// Create KeyPair
-    	KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance("RSA");
-    	keyPairGenerator.initialize(2048, new SecureRandom());
-    	KeyPair keyPair = keyPairGenerator.generateKeyPair();
-    	
-    	Date currentDate = new Date();
-    	Date expiryDate = new Date(currentDate.getTime() + 365L * 24L * 60L * 60L * 1000L);
-    	
-    	// Create X509Certificate
-    	X509v3CertificateBuilder certBuilder =
-    			new X509v3CertificateBuilder(new X500Name(RFC4519Style.INSTANCE, issuerName), serial, currentDate, expiryDate, 
-    					new X500Name(RFC4519Style.INSTANCE, subjectName), SubjectPublicKeyInfo.getInstance(keyPair.getPublic().getEncoded()));
-    	ContentSigner contentSigner = new JcaContentSignerBuilder("SHA256WithRSAEncryption").build(keyPair.getPrivate());
-    	X509Certificate certificate = new JcaX509CertificateConverter().getCertificate(certBuilder.build(contentSigner));
-    	
-    	// Store Private Key + Certificate in Keystore
-    	KeyStore keystore = KeyStore.getInstance(KeyStore.getDefaultType());
-    	keystore.load(null, keystorePassword.toCharArray());
-    	keystore.setKeyEntry(keystoreAlias, keyPair.getPrivate(), keyPassword.toCharArray(), new Certificate[] {certificate});
-    	
-    	File keystoreFile = File.createTempFile("kafkakeystore", ".jks");
-    	try (OutputStream output = new FileOutputStream(keystoreFile)) {
-    		keystore.store(output, keystorePassword.toCharArray());
-    	}
-    	
-    	// Now store the Certificate in the truststore
-    	trustStore.setCertificateEntry(keystoreAlias, certificate);
-    	
-    	return keystoreFile.getPath();
-    	
-    }
-
-	static void createSomeTopics(Properties adminProps) {
-		try (AdminClient adminClient = AdminClient.create(adminProps)) {
-			adminClient.createTopics(Arrays.asList(
-					new NewTopic("test", 1, (short) 1),
-					new NewTopic("dev", 1, (short) 1)
-			));
-		}
-	}
-}
diff --git a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/RangerAdminClientImpl.java b/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/RangerAdminClientImpl.java
deleted file mode 100644
index 9117c6457..000000000
--- a/plugin-kafka/src/test/java/org/apache/ranger/authorization/kafka/authorizer/RangerAdminClientImpl.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.ranger.authorization.kafka.authorizer;
-
-import java.io.File;
-import java.nio.file.FileSystems;
-import java.nio.file.Files;
-import java.util.List;
-
-import org.apache.ranger.admin.client.AbstractRangerAdminClient;
-import org.apache.ranger.plugin.util.ServicePolicies;
-import org.apache.ranger.plugin.util.ServiceTags;
-
-/**
- * A test implementation of the RangerAdminClient interface that just reads policies in from a file and returns them
- */
-public class RangerAdminClientImpl extends AbstractRangerAdminClient {
-    private final static String cacheFilename = "kafka-policies.json";
-    private final static String tagFilename = "kafka-policies-tag.json";
-
-    public ServicePolicies getServicePoliciesIfUpdated(long lastKnownVersion, long lastActivationTimeInMillis) throws Exception {
-
-        String basedir = System.getProperty("basedir");
-        if (basedir == null) {
-            basedir = new File(".").getCanonicalPath();
-        }
-
-        java.nio.file.Path cachePath = FileSystems.getDefault().getPath(basedir, "/src/test/resources/" + cacheFilename);
-        byte[] cacheBytes = Files.readAllBytes(cachePath);
-
-        return gson.fromJson(new String(cacheBytes), ServicePolicies.class);
-    }
-
-    public ServiceTags getServiceTagsIfUpdated(long lastKnownVersion, long lastActivationTimeInMillis) throws Exception {
-        String basedir = System.getProperty("basedir");
-        if (basedir == null) {
-            basedir = new File(".").getCanonicalPath();
-        }
-
-        java.nio.file.Path cachePath = FileSystems.getDefault().getPath(basedir, "/src/test/resources/" + tagFilename);
-        byte[] cacheBytes = Files.readAllBytes(cachePath);
-
-        return gson.fromJson(new String(cacheBytes), ServiceTags.class);
-    }
-
-    public List<String> getTagTypes(String tagTypePattern) throws Exception {
-        return null;
-    }
-
-    
-}
\ No newline at end of file
diff --git a/plugin-kafka/src/test/resources/kafka-policies-tag.json b/plugin-kafka/src/test/resources/kafka-policies-tag.json
deleted file mode 100644
index c85e13344..000000000
--- a/plugin-kafka/src/test/resources/kafka-policies-tag.json
+++ /dev/null
@@ -1,37 +0,0 @@
-{
-  "op": "add_or_update",
-  "serviceName": "cl1_kafka",
-  "tagVersion": 2,
-  "tagDefinitions": {},
-  "tags": {
-    "3": {
-      "type": "MessagesTag",
-      "owner": 0,
-      "attributes": {},
-      "id": 3,
-      "isEnabled": true,
-      "version": 1
-    }
-  },
-  "serviceResources": [
-    {
-      "resourceElements": {
-        "topic": {
-          "values": [
-            "messages"
-          ],
-          "isExcludes": false,
-          "isRecursive": true
-        }
-      },
-      "id": 3,
-      "isEnabled": true,
-      "version": 2
-    }
-  ],
-  "resourceToTagIds": {
-    "3": [
-      3
-    ]
-  }
-}
\ No newline at end of file
diff --git a/plugin-kafka/src/test/resources/kafka-policies.json b/plugin-kafka/src/test/resources/kafka-policies.json
deleted file mode 100644
index 70c978cce..000000000
--- a/plugin-kafka/src/test/resources/kafka-policies.json
+++ /dev/null
@@ -1,1518 +0,0 @@
-{
-  "serviceName": "cl1_kafka",
-  "serviceId": 7,
-  "policyVersion": 5,
-  "policyUpdateTime": "20170220-14:17:37.000-+0000",
-  "policies": [
-    {
-      "service": "cl1_kafka",
-      "name": "all - cluster",
-      "policyType": 0,
-      "description": "Policy for all - cluster",
-      "isAuditEnabled": true,
-      "resources": {
-        "cluster": {
-          "values": [
-            "*"
-          ],
-          "isExcludes": false,
-          "isRecursive": false
-        }
-      },
-      "policyItems": [
-        {
-          "accesses": [
-            {
-              "type": "publish",
-              "isAllowed": true
-            },
-            {
-              "type": "consume",
-              "isAllowed": true
-            },
-            {
-              "type": "configure",
-              "isAllowed": true
-            },
-            {
-              "type": "describe",
-              "isAllowed": true
-            },
-            {
-              "type": "create",
-              "isAllowed": true
-            },
-            {
-              "type": "delete",
-              "isAllowed": true
-            },
-            {
-              "type": "kafka_admin",
-              "isAllowed": true
-            },
-            {
-              "type": "idempotent_write",
-              "isAllowed": true
-            },
-            {
-              "type": "describe_configs",
-              "isAllowed": true
-            },
-            {
-              "type": "alter_configs",
-              "isAllowed": true
-            },
-            {
-              "type": "cluster_action",
-              "isAllowed": true
-            }
-          ],
-          "users": [
-            "admin","kafka"
-          ],
-          "groups": [
-            "IT"
-          ],
-          "conditions": [],
-          "delegateAdmin": true
-        }
-      ],
-      "denyPolicyItems": [],
-      "allowExceptions": [],
-      "denyExceptions": [],
-      "dataMaskPolicyItems": [],
-      "rowFilterPolicyItems": [],
-      "id": 40,
-      "isEnabled": true,
-      "version": 2
-    },
-    {
-      "service": "cl1_kafka",
-      "name": "all - topic",
-      "policyType": 0,
-      "description": "Policy for all - topic",
-      "isAuditEnabled": true,
-      "resources": {
-        "topic": {
-          "values": [
-            "*"
-          ],
-          "isExcludes": false,
-          "isRecursive": false
-        }
-      },
-      "policyItems": [
-        {
-          "accesses": [
-            {
-              "type": "publish",
-              "isAllowed": true
-            },
-            {
-              "type": "consume",
-              "isAllowed": true
-            },
-            {
-              "type": "configure",
-              "isAllowed": true
-            },
-            {
-              "type": "describe",
-              "isAllowed": true
-            },
-            {
-              "type": "create",
-              "isAllowed": true
-            },
-            {
-              "type": "delete",
-              "isAllowed": true
-            },
-            {
-              "type": "kafka_admin",
-              "isAllowed": true
-            },
-            {
-              "type": "idempotent_write",
-              "isAllowed": true
-            },
-            {
-              "type": "describe_configs",
-              "isAllowed": true
-            },
-            {
-              "type": "alter_configs",
-              "isAllowed": true
-            },
-            {
-              "type": "cluster_action",
-              "isAllowed": true
-            }
-          ],
-          "users": [
-            "admin","kafka", "client"
-          ],
-          "groups": [
-            "IT"
-          ],
-          "conditions": [],
-          "delegateAdmin": true
-        }
-      ],
-      "denyPolicyItems": [],
-      "allowExceptions": [],
-      "denyExceptions": [],
-      "dataMaskPolicyItems": [],
-      "rowFilterPolicyItems": [],
-      "id": 18,
-      "isEnabled": true,
-      "version": 2
-    },
-    {
-      "service": "cl1_kafka",
-      "name": "TestPolicy",
-      "policyType": 0,
-      "description": "",
-      "isAuditEnabled": true,
-      "resources": {
-        "topic": {
-          "values": [
-            "test"
-          ],
-          "isExcludes": false,
-          "isRecursive": false
-        }
-      },
-      "policyItems": [
-        {
-          "accesses": [
-            {
-              "type": "publish",
-              "isAllowed": true
-            },
-            {
-              "type": "consume",
-              "isAllowed": true
-            },
-            {
-              "type": "describe",
-              "isAllowed": true
-            },
-            {
-              "type": "idempotent_write",
-              "isAllowed": true
-            },
-            {
-              "type": "describe_configs",
-              "isAllowed": true
-            },
-            {
-              "type": "alter_configs",
-              "isAllowed": true
-            },
-            {
-              "type": "cluster_action",
-              "isAllowed": true
-            }
-          ],
-          "users": [],
-          "groups": [
-            "public"
-          ],
-          "conditions": [],
-          "delegateAdmin": false
-        }
-      ],
-      "denyPolicyItems": [],
-      "allowExceptions": [],
-      "denyExceptions": [],
-      "dataMaskPolicyItems": [],
-      "rowFilterPolicyItems": [],
-      "id": 19,
-      "isEnabled": true,
-      "version": 1
-    },
-    {
-      "service": "cl1_kafka",
-      "name": "ClusterLevelPolicy",
-      "policyType": 0,
-      "description": "",
-      "isAuditEnabled": true,
-      "resources": {
-        "topic": {
-          "values": [
-            "test"
-          ],
-          "isExcludes": false,
-          "isRecursive": false
-        }
-      },
-      "policyItems": [
-        {
-          "accesses": [
-            {
-              "type": "publish",
-              "isAllowed": true
-            },
-            {
-              "type": "consume",
-              "isAllowed": true
-            },
-            {
-              "type": "describe",
-              "isAllowed": true
-            },
-            {
-              "type": "idempotent_write",
-              "isAllowed": true
-            },
-            {
-              "type": "describe_configs",
-              "isAllowed": true
-            },
-            {
-              "type": "alter_configs",
-              "isAllowed": true
-            },
-            {
-              "type": "cluster_action",
-              "isAllowed": true
-            }
-          ],
-          "users": ["kafka"],
-          "groups": [],
-          "conditions": [],
-          "delegateAdmin": false
-        }
-      ],
-      "denyPolicyItems": [],
-      "allowExceptions": [],
-      "denyExceptions": [],
-      "dataMaskPolicyItems": [],
-      "rowFilterPolicyItems": [],
-      "id": 25,
-      "isEnabled": true,
-      "version": 1
-    },
-    {
-      "service": "cl1_kafka",
-      "name": "DevPolicy",
-      "policyType": 0,
-      "description": "",
-      "isAuditEnabled": true,
-      "resources": {
-        "topic": {
-          "values": [
-            "dev"
-          ],
-          "isExcludes": false,
-          "isRecursive": false
-        }
-      },
-      "policyItems": [
-        {
-          "accesses": [
-            {
-              "type": "consume",
-              "isAllowed": true
-            },
-            {
-              "type": "describe",
-              "isAllowed": true
-            }
-          ],
-          "users": [],
-          "groups": [
-            "public"
-          ],
-          "conditions": [],
-          "delegateAdmin": false
-        }
-      ],
-      "denyPolicyItems": [],
-      "allowExceptions": [],
-      "denyExceptions": [],
-      "dataMaskPolicyItems": [],
-      "rowFilterPolicyItems": [],
-      "id": 30,
-      "isEnabled": true,
-      "version": 1
-    },
-    {
-      "service": "cl1_kafka",
-      "name": "DelegationToken Policy",
-      "policyType": 0,
-      "description": "DelegationTokenPolicy",
-      "isAuditEnabled": true,
-      "resources": {
-        "delegationtoken": {
-          "values": [
-            "*"
-          ],
-          "isExcludes": false,
-          "isRecursive": false
-        }
-      },
-      "policyItems": [
-        {
-          "accesses": [
-            {
-              "type": "publish",
-              "isAllowed": true
-            },
-            {
-              "type": "consume",
-              "isAllowed": true
-            },
-            {
-              "type": "configure",
-              "isAllowed": true
-            },
-            {
-              "type": "describe",
-              "isAllowed": true
-            },
-            {
-              "type": "create",
-              "isAllowed": true
-            },
-            {
-              "type": "delete",
-              "isAllowed": true
-            },
-            {
-              "type": "kafka_admin",
-              "isAllowed": true
-            },
-            {
-              "type": "idempotent_write",
-              "isAllowed": true
-            },
-            {
-              "type": "describe_configs",
-              "isAllowed": true
-            },
-            {
-              "type": "alter_configs",
-              "isAllowed": true
-            }
-          ],
-          "users": [
-            "admin","kafka", "client"
-          ],
-          "groups": [
-            "IT"
-          ],
-          "conditions": [],
-          "delegateAdmin": true
-        }
-      ],
-      "denyPolicyItems": [],
-      "allowExceptions": [],
-      "denyExceptions": [],
-      "dataMaskPolicyItems": [],
-      "rowFilterPolicyItems": [],
-      "id": 31,
-      "isEnabled": true,
-      "version": 2
-    },
-    {
-      "service": "cl1_kafka",
-      "name": "ConsumerGroup Policy",
-      "policyType": 0,
-      "description": "ConsumerGroup Policy",
-      "isAuditEnabled": true,
-      "resources": {
-        "consumergroup": {
-          "values": [
-            "*"
-          ],
-          "isExcludes": false,
-          "isRecursive": false
-        }
-      },
-      "policyItems": [
-        {
-          "accesses": [
-            {
-              "type": "consume",
-              "isAllowed": true
-            },
-            {
-              "type": "describe",
-              "isAllowed": true
-            },
-            {
-              "type": "delete",
-              "isAllowed": true
-            }
-          ],
-          "users": [
-            "admin","kafka", "client"
-          ],
-          "groups": [
-            "IT"
-          ],
-          "conditions": [],
-          "delegateAdmin": true
-        }
-      ],
-      "denyPolicyItems": [],
-      "allowExceptions": [],
-      "denyExceptions": [],
-      "dataMaskPolicyItems": [],
-      "rowFilterPolicyItems": [],
-      "id": 32,
-      "isEnabled": true,
-      "version": 2
-    }
-  ],
-  "serviceDef": {
-    "name": "kafka",
-    "implClass": "org.apache.ranger.services.kafka.RangerServiceKafka",
-    "label": "Kafka",
-    "description": "Apache Kafka",
-    "options": {},
-    "configs": [
-      {
-        "itemId": 1,
-        "name": "username",
-        "type": "string",
-        "mandatory": true,
-        "label": "Username"
-      },
-      {
-        "itemId": 2,
-        "name": "password",
-        "type": "password",
-        "mandatory": true,
-        "label": "Password"
-      },
-      {
-        "itemId": 3,
-        "name": "zookeeper.connect",
-        "type": "string",
-        "mandatory": true,
-        "defaultValue": "localhost:2181",
-        "label": "Zookeeper Connect String"
-      },
-      {
-        "itemId": 4,
-        "name": "commonNameForCertificate",
-        "type": "string",
-        "mandatory": false,
-        "label": "Ranger Plugin SSL CName"
-      }
-    ],
-    "resources": [
-      {
-        "itemId": 1,
-        "name": "topic",
-        "type": "string",
-        "level": 1,
-        "mandatory": true,
-        "lookupSupported": true,
-        "recursiveSupported": false,
-        "excludesSupported": true,
-        "matcher": "org.apache.ranger.plugin.resourcematcher.RangerDefaultResourceMatcher",
-        "matcherOptions": {
-          "wildCard": "true",
-          "ignoreCase": "true"
-        },
-        "validationRegEx": "",
-        "validationMessage": "",
-        "uiHint": "",
-        "label": "Topic",
-        "description": "Topic"
-      },
-      {
-        "itemId":2,
-        "name":"transactionalid",
-        "type":"string",
-        "level":1,
-        "mandatory":true,
-        "lookupSupported":false,
-        "recursiveSupported":false,
-        "excludesSupported":true,
-        "matcher":"org.apache.ranger.plugin.resourcematcher.RangerDefaultResourceMatcher",
-        "matcherOptions":{
-          "wildCard":true,
-          "ignoreCase":true
-        },
-        "validationRegEx":"",
-        "validationMessage":"",
-        "uiHint":"",
-        "label":"Transactional Id",
-        "description":"Transactional Id"
-      },
-      {
-        "itemId":3,
-        "name":"cluster",
-        "type":"string",
-        "level":1,
-        "mandatory":true,
-        "lookupSupported":false,
-        "recursiveSupported":false,
-        "excludesSupported":true,
-        "matcher":"org.apache.ranger.plugin.resourcematcher.RangerDefaultResourceMatcher",
-        "matcherOptions":{
-          "wildCard":true,
-          "ignoreCase":true
-        },
-        "validationRegEx":"",
-        "validationMessage":"",
-        "uiHint":"",
-        "label":"Cluster",
-        "description":"Cluster"
-      },
-      {
-        "itemId":4,
-        "name":"delegationtoken",
-        "type":"string",
-        "level":1,
-        "mandatory":true,
-        "lookupSupported":false,
-        "recursiveSupported":false,
-        "excludesSupported":true,
-        "matcher":"org.apache.ranger.plugin.resourcematcher.RangerDefaultResourceMatcher",
-        "matcherOptions":{
-          "wildCard":true,
-          "ignoreCase":true
-        },
-        "validationRegEx":"",
-        "validationMessage":"",
-        "uiHint":"",
-        "label":"Delegation Token",
-        "description":"Delegation Token"
-      },
-      {
-        "itemId":5,
-        "name":"consumergroup",
-        "type":"string",
-        "level":1,
-        "mandatory":true,
-        "lookupSupported":false,
-        "recursiveSupported":false,
-        "excludesSupported":true,
-        "matcher":"org.apache.ranger.plugin.resourcematcher.RangerDefaultResourceMatcher",
-        "matcherOptions":{
-          "wildCard":true,
-          "ignoreCase":true
-        },
-        "validationRegEx":"",
-        "validationMessage":"",
-        "uiHint":"",
-        "label":"Consumer Group",
-        "description":"Consumer Group"
-      }
-    ],
-    "accessTypes": [
-      {
-        "itemId": 1,
-        "name": "publish",
-        "label": "Publish",
-        "impliedGrants": [
-          "describe"
-        ]
-      },
-      {
-        "itemId": 2,
-        "name": "consume",
-        "label": "Consume",
-        "impliedGrants": [
-          "describe"
-        ]
-      },
-      {
-        "itemId": 5,
-        "name": "configure",
-        "label": "Configure",
-        "impliedGrants": [
-          "describe"
-        ]
-      },
-      {
-        "itemId": 6,
-        "name": "describe",
-        "label": "Describe",
-        "impliedGrants": []
-      },
-      {
-        "itemId": 8,
-        "name": "create",
-        "label": "Create",
-        "impliedGrants": []
-      },
-      {
-        "itemId": 9,
-        "name": "delete",
-        "label": "Delete",
-        "impliedGrants": ["describe"]
-      },
-      {
-        "itemId": 7,
-        "name": "kafka_admin",
-        "label": "Kafka Admin",
-        "impliedGrants": [
-          "publish",
-          "consume",
-          "configure",
-          "describe",
-          "create",
-          "delete",
-          "describe_configs",
-          "alter_configs",
-          "idempotent_write",
-          "cluster_action"
-        ]
-      },
-      {
-        "itemId":10,
-        "name":"idempotent_write",
-        "label":"Idempotent Write"
-      },
-      {
-        "itemId":11,
-        "name":"describe_configs",
-        "label":"Describe Configs"
-      },
-      {
-        "itemId":12,
-        "name":"alter_configs",
-        "label":"Alter Configs",
-        "impliedGrants":[
-          "describe_configs"
-        ]
-      },
-      {
-        "itemId":13,
-        "name":"cluster_action",
-        "label":"Cluster Action"
-      }
-    ],
-    "policyConditions": [
-      {
-        "itemId": 1,
-        "name": "ip-range",
-        "evaluator": "org.apache.ranger.plugin.conditionevaluator.RangerIpMatcher",
-        "evaluatorOptions": {},
-        "validationRegEx": "",
-        "validationMessage": "",
-        "uiHint": "",
-        "label": "IP Address Range",
-        "description": "IP Address Range"
-      }
-    ],
-    "contextEnrichers": [],
-    "enums": [],
-    "dataMaskDef": {
-      "maskTypes": [],
-      "accessTypes": [],
-      "resources": []
-    },
-    "rowFilterDef": {
-      "accessTypes": [],
-      "resources": []
-    },
-    "id": 9,
-    "guid": "be2bf354-612e-495a-85d2-9899a2f60947",
-    "isEnabled": true,
-    "createTime": "20170217-11:41:32.000-+0000",
-    "updateTime": "20170217-11:41:32.000-+0000",
-    "version": 1
-  },
-  "auditMode": "audit-default",
-  "tagPolicies": {
-    "serviceName": "KafkaTagService",
-    "serviceId": 5,
-    "policyVersion": 8,
-    "policyUpdateTime": "20170220-14:17:29.000-+0000",
-    "policies": [
-      {
-        "service": "KafkaTagService",
-        "name": "EXPIRES_ON",
-        "policyType": 0,
-        "description": "Policy for data with EXPIRES_ON tag",
-        "isAuditEnabled": true,
-        "resources": {
-          "tag": {
-            "values": [
-              "EXPIRES_ON"
-            ],
-            "isExcludes": false,
-            "isRecursive": false
-          }
-        },
-        "policyItems": [],
-        "denyPolicyItems": [
-          {
-            "accesses": [
-              {
-                "type": "hdfs:read",
-                "isAllowed": true
-              },
-              {
-                "type": "hdfs:write",
-                "isAllowed": true
-              },
-              {
-                "type": "hdfs:execute",
-                "isAllowed": true
-              },
-              {
-                "type": "hbase:read",
-                "isAllowed": true
-              },
-              {
-                "type": "hbase:write",
-                "isAllowed": true
-              },
-              {
-                "type": "hbase:create",
-                "isAllowed": true
-              },
-              {
-                "type": "hbase:admin",
-                "isAllowed": true
-              },
-              {
-                "type": "hive:select",
-                "isAllowed": true
-              },
-              {
-                "type": "hive:update",
-                "isAllowed": true
-              },
-              {
-                "type": "hive:create",
-                "isAllowed": true
-              },
-              {
-                "type": "hive:drop",
-                "isAllowed": true
-              },
-              {
-                "type": "hive:alter",
-                "isAllowed": true
-              },
-              {
-                "type": "hive:index",
-                "isAllowed": true
-              },
-              {
-                "type": "hive:lock",
-                "isAllowed": true
-              },
-              {
-                "type": "hive:all",
-                "isAllowed": true
-              },
-              {
-                "type": "yarn:submit-app",
-                "isAllowed": true
-              },
-              {
-                "type": "yarn:admin-queue",
-                "isAllowed": true
-              },
-              {
-                "type": "knox:allow",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:submitTopology",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:fileUpload",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:fileDownload",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:killTopology",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:rebalance",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:activate",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:deactivate",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:getTopologyConf",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:getTopology",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:getUserTopology",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:getTopologyInfo",
-                "isAllowed": true
-              },
-              {
-                "type": "storm:uploadNewCredentials",
-                "isAllowed": true
-              },
-              {
-                "type": "kms:create",
-                "isAllowed": true
-              },
-              {
-                "type": "kms:delete",
-                "isAllowed": true
-              },
-              {
-                "type": "kms:rollover",
-                "isAllowed": true
-              },
-              {
-                "type": "kms:setkeymaterial",
-                "isAllowed": true
-              },
-              {
-                "type": "kms:get",
-                "isAllowed": true
-              },
-              {
-                "type": "kms:getkeys",
-                "isAllowed": true
-              },
-              {
-                "type": "kms:getmetadata",
-                "isAllowed": true
-              },
-              {
-                "type": "kms:generateeek",
-                "isAllowed": true
-              },
-              {
-                "type": "kms:decrypteek",
-                "isAllowed": true
-              },
-              {
-                "type": "solr:query",
-                "isAllowed": true
-              },
-              {
-                "type": "solr:update",
-                "isAllowed": true
-              },
-              {
-                "type": "solr:others",
-                "isAllowed": true
-              },
-              {
-                "type": "solr:solr_admin",
-                "isAllowed": true
-              },
-              {
-                "type": "kafka:publish",
-                "isAllowed": true
-              },
-              {
-                "type": "kafka:consume",
-                "isAllowed": true
-              },
-              {
-                "type": "kafka:configure",
-                "isAllowed": true
-              },
-              {
-                "type": "kafka:describe",
-                "isAllowed": true
-              },
-              {
-                "type": "kafka:create",
-                "isAllowed": true
-              },
-              {
-                "type": "kafka:delete",
-                "isAllowed": true
-              },
-              {
-                "type": "kafka:kafka_admin",
-                "isAllowed": true
-              },
-              {
-                "type": "kafka:cluster_action",
-                "isAllowed": true
-              },
-              {
-                "type": "atlas:read",
-                "isAllowed": true
-              },
-              {
-                "type": "atlas:create",
-                "isAllowed": true
-              },
-              {
-                "type": "atlas:update",
-                "isAllowed": true
-              },
-              {
-                "type": "atlas:delete",
-                "isAllowed": true
-              },
-              {
-                "type": "atlas:all",
-                "isAllowed": true
-              }
-            ],
-            "users": [],
-            "groups": [
-              "public"
-            ],
-            "conditions": [
-              {
-                "type": "accessed-after-expiry",
-                "values": [
-                  "yes"
-                ]
-              }
-            ],
-            "delegateAdmin": false
-          }
-        ],
-        "allowExceptions": [],
-        "denyExceptions": [],
-        "dataMaskPolicyItems": [],
-        "rowFilterPolicyItems": [],
-        "id": 10,
-        "isEnabled": true,
-        "version": 1
-      },
-      {
-        "service": "KafkaTagService",
-        "name": "MessagesPolicy",
-        "policyType": 0,
-        "description": "",
-        "isAuditEnabled": true,
-        "resources": {
-          "tag": {
-            "values": [
-              "MessagesTag"
-            ],
-            "isExcludes": false,
-            "isRecursive": false
-          }
-        },
-        "policyItems": [
-          {
-            "accesses": [
-              {
-                "type": "kafka:consume",
-                "isAllowed": true
-              },
-              {
-                "type": "kafka:describe",
-                "isAllowed": true
-              }
-            ],
-            "users": [],
-            "groups": [
-              "public"
-            ],
-            "conditions": [],
-            "delegateAdmin": false
-          }
-        ],
-        "denyPolicyItems": [],
-        "allowExceptions": [],
-        "denyExceptions": [],
-        "dataMaskPolicyItems": [],
-        "rowFilterPolicyItems": [],
-        "id": 20,
-        "isEnabled": true,
-        "version": 1
-      }
-    ],
-    "serviceDef": {
-      "name": "tag",
-      "implClass": "org.apache.ranger.services.tag.RangerServiceTag",
-      "label": "TAG",
-      "description": "TAG Service Definition",
-      "options": {
-        "ui.pages": "tag-based-policies"
-      },
-      "configs": [],
-      "resources": [
-        {
-          "itemId": 1,
-          "name": "tag",
-          "type": "string",
-          "level": 1,
-          "mandatory": true,
-          "lookupSupported": true,
-          "recursiveSupported": false,
-          "excludesSupported": false,
-          "matcher": "org.apache.ranger.plugin.resourcematcher.RangerDefaultResourceMatcher",
-          "matcherOptions": {
-            "wildCard": "false",
-            "ignoreCase": "false"
-          },
-          "validationRegEx": "",
-          "validationMessage": "",
-          "uiHint": "{ \"singleValue\":true }",
-          "label": "TAG",
-          "description": "TAG"
-        }
-      ],
-      "accessTypes": [
-        {
-          "itemId": 1002,
-          "name": "hdfs:read",
-          "label": "Read",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 1003,
-          "name": "hdfs:write",
-          "label": "Write",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 1004,
-          "name": "hdfs:execute",
-          "label": "Execute",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 2003,
-          "name": "hbase:read",
-          "label": "Read",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 2004,
-          "name": "hbase:write",
-          "label": "Write",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 2005,
-          "name": "hbase:create",
-          "label": "Create",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 2006,
-          "name": "hbase:admin",
-          "label": "Admin",
-          "impliedGrants": [
-            "hbase:read",
-            "hbase:write",
-            "hbase:create"
-          ]
-        },
-        {
-          "itemId": 3004,
-          "name": "hive:select",
-          "label": "select",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 3005,
-          "name": "hive:update",
-          "label": "update",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 3006,
-          "name": "hive:create",
-          "label": "Create",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 3007,
-          "name": "hive:drop",
-          "label": "Drop",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 3008,
-          "name": "hive:alter",
-          "label": "Alter",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 3009,
-          "name": "hive:index",
-          "label": "Index",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 3010,
-          "name": "hive:lock",
-          "label": "Lock",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 3011,
-          "name": "hive:all",
-          "label": "All",
-          "impliedGrants": [
-            "hive:select",
-            "hive:update",
-            "hive:create",
-            "hive:drop",
-            "hive:alter",
-            "hive:index",
-            "hive:lock"
-          ]
-        },
-        {
-          "itemId": 4005,
-          "name": "yarn:submit-app",
-          "label": "submit-app",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 4006,
-          "name": "yarn:admin-queue",
-          "label": "admin-queue",
-          "impliedGrants": [
-            "yarn:submit-app"
-          ]
-        },
-        {
-          "itemId": 5006,
-          "name": "knox:allow",
-          "label": "Allow",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 6007,
-          "name": "storm:submitTopology",
-          "label": "Submit Topology",
-          "impliedGrants": [
-            "storm:fileUpload",
-            "storm:fileDownload"
-          ]
-        },
-        {
-          "itemId": 6008,
-          "name": "storm:fileUpload",
-          "label": "File Upload",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 6011,
-          "name": "storm:fileDownload",
-          "label": "File Download",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 6012,
-          "name": "storm:killTopology",
-          "label": "Kill Topology",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 6013,
-          "name": "storm:rebalance",
-          "label": "Rebalance",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 6014,
-          "name": "storm:activate",
-          "label": "Activate",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 6015,
-          "name": "storm:deactivate",
-          "label": "Deactivate",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 6016,
-          "name": "storm:getTopologyConf",
-          "label": "Get Topology Conf",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 6017,
-          "name": "storm:getTopology",
-          "label": "Get Topology",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 6018,
-          "name": "storm:getUserTopology",
-          "label": "Get User Topology",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 6019,
-          "name": "storm:getTopologyInfo",
-          "label": "Get Topology Info",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 6020,
-          "name": "storm:uploadNewCredentials",
-          "label": "Upload New Credential",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 7008,
-          "name": "kms:create",
-          "label": "Create",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 7009,
-          "name": "kms:delete",
-          "label": "Delete",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 7010,
-          "name": "kms:rollover",
-          "label": "Rollover",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 7011,
-          "name": "kms:setkeymaterial",
-          "label": "Set Key Material",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 7012,
-          "name": "kms:get",
-          "label": "Get",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 7013,
-          "name": "kms:getkeys",
-          "label": "Get Keys",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 7014,
-          "name": "kms:getmetadata",
-          "label": "Get Metadata",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 7015,
-          "name": "kms:generateeek",
-          "label": "Generate EEK",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 7016,
-          "name": "kms:decrypteek",
-          "label": "Decrypt EEK",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 8108,
-          "name": "solr:query",
-          "label": "Query",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 8208,
-          "name": "solr:update",
-          "label": "Update",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 8308,
-          "name": "solr:others",
-          "label": "Others",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 8908,
-          "name": "solr:solr_admin",
-          "label": "Solr Admin",
-          "impliedGrants": [
-            "solr:query",
-            "solr:update",
-            "solr:others"
-          ]
-        },
-        {
-          "itemId": 9010,
-          "name": "kafka:publish",
-          "label": "Publish",
-          "impliedGrants": [
-            "kafka:describe"
-          ]
-        },
-        {
-          "itemId": 9011,
-          "name": "kafka:consume",
-          "label": "Consume",
-          "impliedGrants": [
-            "kafka:describe"
-          ]
-        },
-        {
-          "itemId": 9014,
-          "name": "kafka:configure",
-          "label": "Configure",
-          "impliedGrants": [
-            "kafka:describe"
-          ]
-        },
-        {
-          "itemId": 9015,
-          "name": "kafka:describe",
-          "label": "Describe",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 9017,
-          "name": "kafka:create",
-          "label": "Create",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 9018,
-          "name": "kafka:delete",
-          "label": "Delete",
-          "impliedGrants": [
-            "kafka:describe"
-          ]
-        },
-        {
-          "itemId": 9016,
-          "name": "kafka:kafka_admin",
-          "label": "Kafka Admin",
-          "impliedGrants": [
-            "kafka:publish",
-            "kafka:consume",
-            "kafka:configure",
-            "kafka:describe",
-            "kafka:create",
-            "kafka:delete",
-            "kafka:describe_configs",
-            "kafka:alter_configs",
-            "kafka:idempotent_write",
-            "kafka:cluster_action"
-          ]
-        },
-        {
-          "itemId":9019,
-          "name":"idempotent_write",
-          "label":"Idempotent Write"
-        },
-        {
-          "itemId":9020,
-          "name":"describe_configs",
-          "label":"Describe Configs"
-        },
-        {
-          "itemId":9021,
-          "name":"alter_configs",
-          "label":"Alter Configs",
-          "impliedGrants":[
-            "describe_configs"
-          ]
-        },
-        {
-          "itemId":9022,
-          "name":"cluster_action",
-          "label":"Cluster Action",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 11012,
-          "name": "atlas:read",
-          "label": "read",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 11013,
-          "name": "atlas:create",
-          "label": "create",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 11014,
-          "name": "atlas:update",
-          "label": "update",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 11015,
-          "name": "atlas:delete",
-          "label": "delete",
-          "impliedGrants": []
-        },
-        {
-          "itemId": 11016,
-          "name": "atlas:all",
-          "label": "All",
-          "impliedGrants": [
-            "atlas:read",
-            "atlas:create",
-            "atlas:update",
-            "atlas:delete"
-          ]
-        }
-      ],
-      "policyConditions": [
-        {
-          "itemId": 1,
-          "name": "accessed-after-expiry",
-          "evaluator": "org.apache.ranger.plugin.conditionevaluator.RangerScriptTemplateConditionEvaluator",
-          "evaluatorOptions": {
-            "scriptTemplate": "ctx.isAccessedAfter(\u0027expiry_date\u0027);"
-          },
-          "uiHint": "{ \"singleValue\":true }",
-          "label": "Accessed after expiry_date (yes/no)?",
-          "description": "Accessed after expiry_date? (yes/no)"
-        }
-      ],
-      "contextEnrichers": [
-        {
-          "itemId": 1,
-          "name": "TagEnricher",
-          "enricher": "org.apache.ranger.plugin.contextenricher.RangerTagEnricher",
-          "enricherOptions": {
-            "tagRetrieverClassName": "org.apache.ranger.plugin.contextenricher.RangerAdminTagRetriever",
-            "tagRefresherPollingInterval": "60000"
-          }
-        }
-      ],
-      "enums": [],
-      "dataMaskDef": {
-        "maskTypes": [],
-        "accessTypes": [],
-        "resources": []
-      },
-      "rowFilterDef": {
-        "accessTypes": [],
-        "resources": []
-      },
-      "id": 100,
-      "guid": "0d047248-baff-4cf9-8e9e-d5d377284b2e",
-      "isEnabled": true,
-      "createTime": "20170217-11:41:33.000-+0000",
-      "updateTime": "20170217-11:41:35.000-+0000",
-      "version": 11
-    },
-    "auditMode": "audit-default"
-  }
-}
\ No newline at end of file
diff --git a/plugin-kafka/src/test/resources/kafka_kerberos.jaas b/plugin-kafka/src/test/resources/kafka_kerberos.jaas
deleted file mode 100644
index 2e83c7cc9..000000000
--- a/plugin-kafka/src/test/resources/kafka_kerberos.jaas
+++ /dev/null
@@ -1,20 +0,0 @@
-
-Server {
-        com.sun.security.auth.module.Krb5LoginModule required refreshKrb5Config=true useKeyTab=true serviceName="kafka"
-        keyTab="<basedir>/target/zookeeper.keytab" storeKey=true principal="zookeeper/localhost";
-};
-
-KafkaServer {
-        com.sun.security.auth.module.Krb5LoginModule required refreshKrb5Config=true useKeyTab=true serviceName="kafka"
-        keyTab="<basedir>/target/kafka.keytab" storeKey=true principal="kafka/localhost";
-};
-
-Client {
-        com.sun.security.auth.module.Krb5LoginModule required refreshKrb5Config=true useKeyTab=true serviceName="kafka"
-        keyTab="<basedir>/target/kafka.keytab" storeKey=true principal="kafka/localhost";
-};
-
-KafkaClient {
-        com.sun.security.auth.module.Krb5LoginModule required refreshKrb5Config=true useKeyTab=true serviceName="kafka"
-        keyTab="<basedir>/target/client.keytab" storeKey=true principal="client";
-};
diff --git a/plugin-kafka/src/test/resources/kafka_plain.jaas b/plugin-kafka/src/test/resources/kafka_plain.jaas
deleted file mode 100644
index 6c090f75c..000000000
--- a/plugin-kafka/src/test/resources/kafka_plain.jaas
+++ /dev/null
@@ -1,13 +0,0 @@
-KafkaServer {
-            org.apache.kafka.common.security.plain.PlainLoginModule required
-            username="admin"
-            password="password"
-            user_admin="password"
-            user_alice="security";
-};
-
-KafkaClient {
-            org.apache.kafka.common.security.plain.PlainLoginModule required
-            username="alice"
-            password="security";
-};
\ No newline at end of file
diff --git a/plugin-kafka/src/test/resources/log4j.properties b/plugin-kafka/src/test/resources/log4j.properties
deleted file mode 100644
index 4ad14dec6..000000000
--- a/plugin-kafka/src/test/resources/log4j.properties
+++ /dev/null
@@ -1,28 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-# Root logger option
-log4j.rootLogger=INFO, stdout
-log4j.logger.kafka=WARN
-log4j.logger.org.apache.zookeeper=WARN
-
-# Direct log messages to stdout
-log4j.appender.stdout=org.apache.log4j.ConsoleAppender
-log4j.appender.stdout.Target=System.out
-log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
-log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{10}:%L - %m%n
diff --git a/plugin-kafka/src/test/resources/ranger-kafka-security.xml b/plugin-kafka/src/test/resources/ranger-kafka-security.xml
deleted file mode 100644
index 9e9b5e1ac..000000000
--- a/plugin-kafka/src/test/resources/ranger-kafka-security.xml
+++ /dev/null
@@ -1,61 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Licensed to the Apache Software Foundation (ASF) under one or more
-  contributor license agreements.  See the NOTICE file distributed with
-  this work for additional information regarding copyright ownership.
-  The ASF licenses this file to You under the Apache License, Version 2.0
-  (the "License"); you may not use this file except in compliance with
-  the License.  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License.
--->
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<configuration xmlns:xi="http://www.w3.org/2001/XInclude">
-	<property>
-		<name>ranger.plugin.kafka.policy.rest.url</name>
-		<value>http://localhost:6080</value>
-		<description>
-			URL to Ranger Admin
-		</description>
-	</property>
-
-	<property>
-		<name>ranger.plugin.kafka.service.name</name>
-		<value>cl1_kafka</value>
-		<description>
-			Name of the Ranger service containing policies for this SampleApp instance
-		</description>
-	</property>
-
-	<property>
-        <name>ranger.plugin.kafka.policy.source.impl</name>
-        <value>org.apache.ranger.authorization.kafka.authorizer.RangerAdminClientImpl</value>
-        <!-- <value>org.apache.ranger.admin.client.RangerAdminRESTClient</value> -->
-        <description>
-            Policy source.
-        </description>
-    </property>
-    
-	<property>
-		<name>ranger.plugin.kafka.policy.pollIntervalMs</name>
-		<value>30000</value>
-		<description>
-			How often to poll for changes in policies?
-		</description>
-	</property>
-
-	<property>
-		<name>ranger.plugin.kafka.policy.cache.dir</name>
-		<value>${project.build.directory}</value>
-		<description>
-			Directory where Ranger policies are cached after successful retrieval from the source
-		</description>
-	</property>
-
-</configuration>
diff --git a/pom.xml b/pom.xml
index 1d93c163b..6f656790f 100644
--- a/pom.xml
+++ b/pom.xml
@@ -247,7 +247,6 @@
                 <module>knox-agent</module>
                 <module>plugin-yarn</module>
                 <module>security-admin</module>
-                <module>plugin-kafka</module>
                 <module>plugin-solr</module>
                 <module>plugin-nifi</module>
                 <module>plugin-nifi-registry</module>
@@ -368,20 +367,6 @@
                 <module>ranger-yarn-plugin-shim</module>
             </modules>
         </profile>
-        <profile>
-            <id>ranger-kafka-plugin</id>
-            <modules>
-                <module>agents-audit</module>
-                <module>agents-common</module>
-                <module>agents-cred</module>
-                <module>agents-installer</module>
-                <module>credentialbuilder</module>
-                <module>ranger-plugin-classloader</module>
-                <module>ranger-util</module>
-                <module>plugin-kafka</module>
-                <module>ranger-kafka-plugin-shim</module>
-            </modules>
-        </profile>
         <profile>
             <id>ranger-solr-plugin</id>
             <modules>
@@ -503,7 +488,6 @@
                 <module>knox-agent</module>
                 <module>plugin-yarn</module>
                 <module>security-admin</module>
-                <module>plugin-kafka</module>
                 <module>plugin-solr</module>
                 <module>plugin-nifi</module>
                 <module>plugin-nifi-registry</module>
@@ -585,7 +569,6 @@
                 <module>knox-agent</module>
                 <module>plugin-yarn</module>
                 <module>security-admin</module>
-                <module>plugin-kafka</module>
                 <module>plugin-solr</module>
                 <module>plugin-nifi</module>
                 <module>plugin-nifi-registry</module>
